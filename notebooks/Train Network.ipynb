{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Ideas\n",
    "- Train a very large CNN-dense network on big computer:\n",
    "    - Use window step size of 1\n",
    "    - Problem: sequences are so long that the model is more likely to overfit than to learn useful things\n",
    "    - Solution: do random masking on data as sort of regularization\n",
    "    - 1D CNNs with smaller stride, followed by just dense should be a decent architecture\n",
    "    - If its not too difficult, do CNN for dim-reduction followed by transformer block\n",
    "- Split a sequence into chunks and do manual feature engineering:\n",
    "    - Pro: Solves the overfitting problem with long sequences\n",
    "    - Pro: trains faster\n",
    "    - Con: Removes one of the main benefits of NNs (automatic feature engineering)\n",
    "    - Con: requires clever and careful feature engineering\n",
    "    - Con: might be more computationally heavy if feat eng is done on the fly\n",
    "- Try transformer/self-attention architecture\n",
    "- Try the feature engineering approach for validation\n",
    "    - Maybe try continuous prediction? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-08T13:04:35.901559Z",
     "start_time": "2019-03-08T13:04:35.502719Z"
    }
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "if not '../' in sys.path:\n",
    "    sys.path.append('../')\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "import dask\n",
    "\n",
    "import importlib"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read in and preprocess data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-08T13:04:35.919975Z",
     "start_time": "2019-03-08T13:04:35.918483Z"
    }
   },
   "outputs": [],
   "source": [
    "data_dir = '../data/'\n",
    "preprocessed_dir = data_dir + 'preprocessed/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-08T13:05:52.134420Z",
     "start_time": "2019-03-08T13:04:36.311163Z"
    }
   },
   "outputs": [],
   "source": [
    "train_data = pd.read_csv(data_dir + 'train.csv',  dtype={'acoustic_data': np.float32, 'time_to_failure': np.float32}).values\n",
    "test_dir = data_dir + 'test/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-08T13:05:54.849995Z",
     "start_time": "2019-03-08T13:05:52.151845Z"
    }
   },
   "outputs": [],
   "source": [
    "## Drop some of the training data for memory efficiency\n",
    "data_frac = 1.0\n",
    "train_data = train_data[:int(data_frac * len(train_data))].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-08T13:05:54.882128Z",
     "start_time": "2019-03-08T13:05:54.865512Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aiwizo/anaconda3/lib/python3.6/site-packages/pandas/core/series.py:3724: FutureWarning: from_csv is deprecated. Please use read_csv(...) instead. Note that some of the default arguments are different, so please refer to the documentation for from_csv when changing your function calls\n",
      "  infer_datetime_format=infer_datetime_format)\n"
     ]
    }
   ],
   "source": [
    "train_desc = pd.Series.from_csv(preprocessed_dir + 'training_data_description.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-08T13:05:54.901081Z",
     "start_time": "2019-03-08T13:05:54.899275Z"
    }
   },
   "outputs": [],
   "source": [
    "def scale(acoustic_data, standard=True):\n",
    "    if not standard:\n",
    "        ## Puts values in range [-1, 1]\n",
    "        acoustic_data = 2 * (acoustic_data - train_desc['mean']) / (train_desc['max'] - train_desc['min'])\n",
    "    else:\n",
    "        acoustic_data = (acoustic_data - train_desc['mean']) / train_desc['std']\n",
    "        \n",
    "    return acoustic_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-08T13:05:56.400043Z",
     "start_time": "2019-03-08T13:05:54.917507Z"
    }
   },
   "outputs": [],
   "source": [
    "train_data[:, 0] = scale(train_data[:, 0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-08T13:06:25.476166Z",
     "start_time": "2019-03-08T13:06:25.445047Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda.\n"
     ]
    }
   ],
   "source": [
    "## Model config\n",
    "config = dict(\n",
    "    \n",
    "    data_dir = data_dir,\n",
    "    use_cuda = torch.cuda.is_available(),\n",
    "    seq_len = 150000,\n",
    "    \n",
    "    ## Training parameters\n",
    "    batch_size = 16,\n",
    "    lr = 0.001,\n",
    "    num_epochs = 20,\n",
    "    clip = 0.1, # Gradient clipping\n",
    "    eval_step = 0.1, # how often to evaluate, per epoch. E.g., 0.5 -> 2 times per epoch\n",
    "    patience = 10, # patience (in nr of evals) for early stopping. If None, will not use early stopping \n",
    "    revert_after_training = True, # If true, reverts model parameters after training to best found during early stopping\n",
    "    \n",
    "    ## Model hyperparameters\n",
    "    model_choice = 0,\n",
    "    optim_choice = 0,\n",
    "    n_filters = [64, 128, 128, 256, 256, 512],\n",
    "    kernel_size = [3000, 50, 10, 5, 5, 5],\n",
    "    conv_stride = [50, 10, 5, 3, 2, 2],\n",
    "    dense_size = [1000],\n",
    "    dropout = 0.3,\n",
    ")\n",
    "\n",
    "device = torch.device(\"cuda\" if config['use_cuda'] else \"cpu\")\n",
    "print(\"Using {}.\".format(device))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-08T13:26:55.901046Z",
     "start_time": "2019-03-08T13:26:55.898324Z"
    }
   },
   "outputs": [],
   "source": [
    "import utils.dataset\n",
    "importlib.reload(utils.dataset)\n",
    "from utils.dataset import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-08T13:27:11.963235Z",
     "start_time": "2019-03-08T13:27:11.942603Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "503,016 train examples.\n",
      "837 valid examples.\n"
     ]
    }
   ],
   "source": [
    "valid_frac = 0.2\n",
    "n_train_data = int(len(train_data) * (1-valid_frac))\n",
    "\n",
    "X_train = train_data[:n_train_data - 150000]\n",
    "X_valid = train_data[n_train_data:]\n",
    "\n",
    "train_dataset = EarthquakeDatasetTrain(X_train, window_step=1000, mask_prob=0.2)\n",
    "valid_dataset = EarthquakeDatasetTrain(X_valid, window_step=150000)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, \n",
    "                          batch_size=config['batch_size'], \n",
    "                          shuffle=True, \n",
    "                          num_workers=4)\n",
    "\n",
    "valid_loader = DataLoader(valid_dataset, \n",
    "                         batch_size=100, \n",
    "                         shuffle=False, \n",
    "                         num_workers=4)\n",
    "\n",
    "print(\"{:,} train examples.\".format(len(train_dataset)))\n",
    "print(\"{:,} valid examples.\".format(len(valid_dataset)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-08T13:27:13.686008Z",
     "start_time": "2019-03-08T13:27:13.646240Z"
    },
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Intermediate sizes:\n",
      "150000 3001\n",
      "3001 301\n",
      "301 61\n",
      "61 21\n",
      "21 11\n",
      "11 6\n"
     ]
    }
   ],
   "source": [
    "import utils.models\n",
    "importlib.reload(utils.models)\n",
    "from utils.models import *\n",
    "\n",
    "import utils.model_wrapper\n",
    "importlib.reload(utils.model_wrapper)\n",
    "from utils.model_wrapper import *\n",
    "model = ModelWrapper(config=config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-08T13:27:15.340685Z",
     "start_time": "2019-03-08T13:27:15.333814Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4,992,353 total parameters\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th># params</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>convs.0.0.weight</td>\n",
       "      <td>192,000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>convs.0.0.bias</td>\n",
       "      <td>64</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>convs.0.1.weight</td>\n",
       "      <td>64</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>convs.0.1.bias</td>\n",
       "      <td>64</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>convs.1.0.weight</td>\n",
       "      <td>409,600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>convs.1.0.bias</td>\n",
       "      <td>128</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>convs.1.1.weight</td>\n",
       "      <td>128</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>convs.1.1.bias</td>\n",
       "      <td>128</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>convs.2.0.weight</td>\n",
       "      <td>163,840</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>convs.2.0.bias</td>\n",
       "      <td>128</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>convs.2.1.weight</td>\n",
       "      <td>128</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>convs.2.1.bias</td>\n",
       "      <td>128</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>convs.3.0.weight</td>\n",
       "      <td>163,840</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>convs.3.0.bias</td>\n",
       "      <td>256</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>convs.3.1.weight</td>\n",
       "      <td>256</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>convs.3.1.bias</td>\n",
       "      <td>256</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>convs.4.0.weight</td>\n",
       "      <td>327,680</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>convs.4.0.bias</td>\n",
       "      <td>256</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>convs.4.1.weight</td>\n",
       "      <td>256</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>convs.4.1.bias</td>\n",
       "      <td>256</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>convs.5.0.weight</td>\n",
       "      <td>655,360</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>convs.5.0.bias</td>\n",
       "      <td>512</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>convs.5.1.weight</td>\n",
       "      <td>512</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>convs.5.1.bias</td>\n",
       "      <td>512</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>dense.0.0.weight</td>\n",
       "      <td>3,072,000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>dense.0.0.bias</td>\n",
       "      <td>1,000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>dense.0.1.weight</td>\n",
       "      <td>1,000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>dense.0.1.bias</td>\n",
       "      <td>1,000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>dense.1.weight</td>\n",
       "      <td>1,000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>dense.1.bias</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                name   # params\n",
       "0   convs.0.0.weight    192,000\n",
       "1     convs.0.0.bias         64\n",
       "2   convs.0.1.weight         64\n",
       "3     convs.0.1.bias         64\n",
       "4   convs.1.0.weight    409,600\n",
       "5     convs.1.0.bias        128\n",
       "6   convs.1.1.weight        128\n",
       "7     convs.1.1.bias        128\n",
       "8   convs.2.0.weight    163,840\n",
       "9     convs.2.0.bias        128\n",
       "10  convs.2.1.weight        128\n",
       "11    convs.2.1.bias        128\n",
       "12  convs.3.0.weight    163,840\n",
       "13    convs.3.0.bias        256\n",
       "14  convs.3.1.weight        256\n",
       "15    convs.3.1.bias        256\n",
       "16  convs.4.0.weight    327,680\n",
       "17    convs.4.0.bias        256\n",
       "18  convs.4.1.weight        256\n",
       "19    convs.4.1.bias        256\n",
       "20  convs.5.0.weight    655,360\n",
       "21    convs.5.0.bias        512\n",
       "22  convs.5.1.weight        512\n",
       "23    convs.5.1.bias        512\n",
       "24  dense.0.0.weight  3,072,000\n",
       "25    dense.0.0.bias      1,000\n",
       "26  dense.0.1.weight      1,000\n",
       "27    dense.0.1.bias      1,000\n",
       "28    dense.1.weight      1,000\n",
       "29      dense.1.bias          1"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summary, n_params = model.get_summary()\n",
    "print(\"{:,} total parameters\".format(n_params))\n",
    "summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-06T12:32:07.047375Z",
     "start_time": "2019-03-06T12:32:02.524177Z"
    },
    "deletable": false,
    "editable": false,
    "run_control": {
     "frozen": true
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## DEBUG\n",
    "config_changes = dict(\n",
    "    num_epochs = 1,\n",
    "    eval_step = 0.001,\n",
    "    patience = 2,\n",
    "    revert_after_training = True,\n",
    "    clip = 0.5,\n",
    "    lr = 0.01,\n",
    ")\n",
    "model.update_config(config_changes)\n",
    "\n",
    "_ = model.train(train_loader, valid_loader, verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-08T13:48:55.057466Z",
     "start_time": "2019-03-08T13:27:20.230535Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------- EPOCH 1/20 ----------\n",
      "\n",
      "New best!\n",
      "Step: 3143/31439\n",
      "Total steps: 3143\n",
      "Training Loss (smooth): 2.980\n",
      "Validation Loss: 3.253\n",
      "Maximum GPU consumption so far: 0.439 [GB]\n",
      "\n",
      "Step: 6286/31439\n",
      "Total steps: 6286\n",
      "Training Loss (smooth): 2.985\n",
      "Validation Loss: 3.268\n",
      "Maximum GPU consumption so far: 0.439 [GB]\n",
      "\n",
      "Step: 9429/31439\n",
      "Total steps: 9429\n",
      "Training Loss (smooth): 2.973\n",
      "Validation Loss: 3.279\n",
      "Maximum GPU consumption so far: 0.439 [GB]\n",
      "\n",
      "New best!\n",
      "Step: 12572/31439\n",
      "Total steps: 12572\n",
      "Training Loss (smooth): 3.044\n",
      "Validation Loss: 3.243\n",
      "Maximum GPU consumption so far: 0.439 [GB]\n",
      "\n",
      "New best!\n",
      "Step: 15715/31439\n",
      "Total steps: 15715\n",
      "Training Loss (smooth): 2.933\n",
      "Validation Loss: 3.242\n",
      "Maximum GPU consumption so far: 0.439 [GB]\n",
      "\n",
      "New best!\n",
      "Step: 18858/31439\n",
      "Total steps: 18858\n",
      "Training Loss (smooth): 2.975\n",
      "Validation Loss: 3.228\n",
      "Maximum GPU consumption so far: 0.439 [GB]\n",
      "\n",
      "New best!\n",
      "Step: 22001/31439\n",
      "Total steps: 22001\n",
      "Training Loss (smooth): 2.933\n",
      "Validation Loss: 3.213\n",
      "Maximum GPU consumption so far: 0.439 [GB]\n",
      "\n",
      "Step: 25144/31439\n",
      "Total steps: 25144\n",
      "Training Loss (smooth): 2.920\n",
      "Validation Loss: 3.243\n",
      "Maximum GPU consumption so far: 0.439 [GB]\n",
      "\n",
      "New best!\n",
      "Step: 28287/31439\n",
      "Total steps: 28287\n",
      "Training Loss (smooth): 2.927\n",
      "Validation Loss: 3.210\n",
      "Maximum GPU consumption so far: 0.439 [GB]\n",
      "\n",
      "Step: 31430/31439\n",
      "Total steps: 31430\n",
      "Training Loss (smooth): 2.956\n",
      "Validation Loss: 3.228\n",
      "Maximum GPU consumption so far: 0.439 [GB]\n",
      "\n",
      "---------- EPOCH 2/20 ----------\n",
      "\n",
      "Step: 3134/31439\n",
      "Total steps: 34573\n",
      "Training Loss (smooth): 2.942\n",
      "Validation Loss: 3.231\n",
      "Maximum GPU consumption so far: 0.439 [GB]\n",
      "\n",
      "Step: 6277/31439\n",
      "Total steps: 37716\n",
      "Training Loss (smooth): 2.877\n",
      "Validation Loss: 3.235\n",
      "Maximum GPU consumption so far: 0.439 [GB]\n",
      "\n",
      "Step: 9420/31439\n",
      "Total steps: 40859\n",
      "Training Loss (smooth): 2.905\n",
      "Validation Loss: 3.220\n",
      "Maximum GPU consumption so far: 0.439 [GB]\n",
      "\n",
      "Step: 12563/31439\n",
      "Total steps: 44002\n",
      "Training Loss (smooth): 2.876\n",
      "Validation Loss: 3.246\n",
      "Maximum GPU consumption so far: 0.439 [GB]\n",
      "\n",
      "New best!\n",
      "Step: 15706/31439\n",
      "Total steps: 47145\n",
      "Training Loss (smooth): 2.915\n",
      "Validation Loss: 3.206\n",
      "Maximum GPU consumption so far: 0.439 [GB]\n",
      "\n",
      "Step: 18849/31439\n",
      "Total steps: 50288\n",
      "Training Loss (smooth): 2.918\n",
      "Validation Loss: 3.215\n",
      "Maximum GPU consumption so far: 0.439 [GB]\n",
      "\n",
      "New best!\n",
      "Step: 21992/31439\n",
      "Total steps: 53431\n",
      "Training Loss (smooth): 2.967\n",
      "Validation Loss: 3.199\n",
      "Maximum GPU consumption so far: 0.439 [GB]\n",
      "\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-37-aaeee4306ae9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate_config\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig_changes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalid_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Preperatory training finished!\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/Kaggle/LANL-Earthquake/utils/model_wrapper.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, train_loader, val_loader, verbose)\u001b[0m\n\u001b[1;32m    149\u001b[0m                     nn.utils.clip_grad_norm_(self.net.parameters(), \n\u001b[1;32m    150\u001b[0m                                              self.config['clip'])\n\u001b[0;32m--> 151\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    152\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    153\u001b[0m                 \u001b[0mtot_step\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/torch/optim/adam.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, closure)\u001b[0m\n\u001b[1;32m     91\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m                 \u001b[0;31m# Decay the first and second moment running average coefficient\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 93\u001b[0;31m                 \u001b[0mexp_avg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmul_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbeta1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mbeta1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     94\u001b[0m                 \u001b[0mexp_avg_sq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmul_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbeta2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maddcmul_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mbeta2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mamsgrad\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "## Preparatory training with higher learning rate and lower gradient clipping\n",
    "config_changes = dict(\n",
    "    num_epochs = 20,\n",
    "    eval_step = 0.1,\n",
    "    patience = 30,\n",
    "    revert_after_training = True,\n",
    "    clip = 0.1,\n",
    "    lr = 0.001,\n",
    ")\n",
    "model.update_config(config_changes)\n",
    "\n",
    "_ = model.train(train_loader, valid_loader, verbose=2)\n",
    "print(\"Preperatory training finished!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## Full training\n",
    "config_changes = dict(\n",
    "    num_epochs = 100,\n",
    "    patience = config['patience'],\n",
    "    revert_after_training = True,\n",
    "    clip = config['clip'],\n",
    "    lr = config['lr'],\n",
    ")\n",
    "model.update_config(config_changes)\n",
    "\n",
    "_ = model.train(train_loader, valid_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-08T10:12:44.346245Z",
     "start_time": "2019-03-08T10:12:44.333999Z"
    }
   },
   "outputs": [],
   "source": [
    "#model.save_state('../checkpoints/', 'model0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-08T10:16:18.972995Z",
     "start_time": "2019-03-08T10:16:18.966331Z"
    }
   },
   "outputs": [],
   "source": [
    "#model = ModelWrapper(pretrained_path='../checkpoints/model0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-08T13:04:19.486279Z",
     "start_time": "2019-03-08T10:52:44.268Z"
    }
   },
   "outputs": [],
   "source": [
    "preds = model.predict(valid_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-08T13:04:19.488562Z",
     "start_time": "2019-03-08T10:52:44.924Z"
    }
   },
   "outputs": [],
   "source": [
    "preds.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-08T13:04:19.490978Z",
     "start_time": "2019-03-08T10:52:45.565Z"
    }
   },
   "outputs": [],
   "source": [
    "preds.std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-08T13:04:19.493275Z",
     "start_time": "2019-03-08T10:52:46.205Z"
    }
   },
   "outputs": [],
   "source": [
    "X_train[1].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-08T13:04:19.495652Z",
     "start_time": "2019-03-08T10:52:46.892Z"
    }
   },
   "outputs": [],
   "source": [
    "X_train[1].std()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-08T09:12:50.611492Z",
     "start_time": "2019-03-08T09:12:50.605023Z"
    }
   },
   "outputs": [],
   "source": [
    "test_dataset = EarthquakeDatasetTest(test_dir)\n",
    "test_loader = DataLoader(test_dataset,\n",
    "                         batch_size=100, \n",
    "                         shuffle=False, \n",
    "                         num_workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-08T09:13:12.359272Z",
     "start_time": "2019-03-08T09:12:50.639849Z"
    }
   },
   "outputs": [],
   "source": [
    "preds, ids = model.predict(test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-08T09:13:12.386814Z",
     "start_time": "2019-03-08T09:13:12.384951Z"
    }
   },
   "outputs": [],
   "source": [
    "submission = pd.DataFrame({\n",
    "    'seg_id' : ids,\n",
    "    'time_to_failure' : preds,\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-08T09:13:12.429427Z",
     "start_time": "2019-03-08T09:13:12.423600Z"
    }
   },
   "outputs": [],
   "source": [
    "submission.to_csv('../submission.csv', index=None)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "230px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
