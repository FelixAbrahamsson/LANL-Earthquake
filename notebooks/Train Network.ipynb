{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Ideas\n",
    "- Train a very large CNN-dense network on big computer:\n",
    "    - Use window step size of 1\n",
    "    - Problem: sequences are so long that the model is more likely to overfit than to learn useful things\n",
    "    - Solution: do random masking on data as sort of regularization\n",
    "    - 1D CNNs with smaller stride, followed by just dense should be a decent architecture\n",
    "    - If its not too difficult, do CNN for dim-reduction followed by transformer block\n",
    "- Split a sequence into chunks and do manual feature engineering:\n",
    "    - Pro: Solves the overfitting problem with long sequences\n",
    "    - Pro: trains faster\n",
    "    - Con: Removes one of the main benefits of NNs (automatic feature engineering)\n",
    "    - Con: requires clever and careful feature engineering\n",
    "    - Con: might be more computationally heavy if feat eng is done on the fly"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-06T15:44:21.024561Z",
     "start_time": "2019-03-06T15:44:20.047816Z"
    }
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "if not '../' in sys.path:\n",
    "    sys.path.append('../')\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "import dask\n",
    "\n",
    "import importlib"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read in and preprocess data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-06T15:44:26.917086Z",
     "start_time": "2019-03-06T15:44:26.914772Z"
    }
   },
   "outputs": [],
   "source": [
    "data_dir = '../data/'\n",
    "preprocessed_dir = data_dir + 'preprocessed/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-06T15:45:59.738723Z",
     "start_time": "2019-03-06T15:44:27.839158Z"
    }
   },
   "outputs": [],
   "source": [
    "train_data = pd.read_csv(data_dir + 'train.csv',  dtype={'acoustic_data': np.float32, 'time_to_failure': np.float32}).values\n",
    "test_dir = preprocessed_dir + 'test/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-06T15:48:13.053043Z",
     "start_time": "2019-03-06T15:48:11.291922Z"
    }
   },
   "outputs": [],
   "source": [
    "## Drop some of the training data for memory efficiency\n",
    "data_frac = 0.5\n",
    "train_data = train_data[:int(data_frac * len(train_data))].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-06T15:48:14.329014Z",
     "start_time": "2019-03-06T15:48:14.303724Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/felix/anaconda3/lib/python3.6/site-packages/pandas/core/series.py:3727: FutureWarning: from_csv is deprecated. Please use read_csv(...) instead. Note that some of the default arguments are different, so please refer to the documentation for from_csv when changing your function calls\n",
      "  infer_datetime_format=infer_datetime_format)\n"
     ]
    }
   ],
   "source": [
    "train_desc = pd.Series.from_csv(preprocessed_dir + 'training_data_description.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-06T15:48:15.384744Z",
     "start_time": "2019-03-06T15:48:15.381882Z"
    }
   },
   "outputs": [],
   "source": [
    "def scale(acoustic_data):\n",
    "    ## Puts values in range [-1, 1]\n",
    "    acoustic_data = 2 * (acoustic_data - train_desc['mean']) / (train_desc['max'] - train_desc['min'])\n",
    "    return acoustic_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-06T15:48:17.077008Z",
     "start_time": "2019-03-06T15:48:17.074114Z"
    }
   },
   "outputs": [],
   "source": [
    "train_data[0] = scale(train_data[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-06T15:48:56.363188Z",
     "start_time": "2019-03-06T15:48:56.329448Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda.\n"
     ]
    }
   ],
   "source": [
    "## Model config\n",
    "config = dict(\n",
    "    \n",
    "    data_dir = data_dir,\n",
    "    use_cuda = torch.cuda.is_available(),\n",
    "    seq_len = 150000,\n",
    "    \n",
    "    ## Training parameters\n",
    "    batch_size = 16,\n",
    "    lr = 0.001,\n",
    "    num_epochs = 20,\n",
    "    clip = 0.1, # Gradient clipping\n",
    "    eval_step = 0.5, # how often to evaluate, per epoch. E.g., 0.5 -> 2 times per epoch\n",
    "    patience = 10, # patience (in nr of evals) for early stopping. If None, will not use early stopping \n",
    "    revert_after_training = True, # If true, reverts model parameters after training to best found during early stopping\n",
    "    \n",
    "    ## Model hyperparameters\n",
    "    model_choice = 0,\n",
    "    optim_choice = 0,\n",
    "    n_filters = [16, 32],\n",
    "    kernel_size = [1000, 50],\n",
    "    conv_stride = [50, 10],\n",
    "    dense_size = 1000,\n",
    "    dropout = 0.3,\n",
    ")\n",
    "\n",
    "device = torch.device(\"cuda\" if config['use_cuda'] else \"cpu\")\n",
    "print(\"Using {}.\".format(device))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-06T16:20:32.432990Z",
     "start_time": "2019-03-06T16:20:32.429007Z"
    }
   },
   "outputs": [],
   "source": [
    "import utils.dataset\n",
    "importlib.reload(utils.dataset)\n",
    "from utils.dataset import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-06T16:20:32.642958Z",
     "start_time": "2019-03-06T16:20:32.620153Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "251,358 train examples.\n",
      "418 valid examples.\n"
     ]
    }
   ],
   "source": [
    "valid_frac = 0.2\n",
    "n_train_data = int(len(train_data) * (1-valid_frac))\n",
    "\n",
    "X_train = train_data[:n_train_data - 150000]\n",
    "X_valid = train_data[n_train_data:]\n",
    "\n",
    "train_dataset = EarthquakeDatasetFull(X_train, window_step=1000)\n",
    "valid_dataset = EarthquakeDatasetFull(X_valid, window_step=150000)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, \n",
    "                          batch_size=config['batch_size'], \n",
    "                          shuffle=True, \n",
    "                          num_workers=4)\n",
    "\n",
    "valid_loader = DataLoader(valid_dataset, \n",
    "                         batch_size=100, \n",
    "                         shuffle=False, \n",
    "                         num_workers=4)\n",
    "\n",
    "print(\"{:,} train examples.\".format(len(train_dataset)))\n",
    "print(\"{:,} valid examples.\".format(len(valid_dataset)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-06T16:19:45.127889Z",
     "start_time": "2019-03-06T16:19:45.035562Z"
    },
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Intermediate sizes:\n",
      "150000 3001\n",
      "3001 301\n"
     ]
    }
   ],
   "source": [
    "import utils.models\n",
    "importlib.reload(utils.models)\n",
    "from utils.models import *\n",
    "\n",
    "import utils.model_wrapper\n",
    "importlib.reload(utils.model_wrapper)\n",
    "from utils.model_wrapper import *\n",
    "model = ModelWrapper(config=config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-06T15:49:01.197662Z",
     "start_time": "2019-03-06T15:49:01.183957Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9,675,745 total parameters\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th># params</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>convs.0.0.weight</td>\n",
       "      <td>16,000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>convs.0.0.bias</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>convs.0.1.weight</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>convs.0.1.bias</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>convs.1.0.weight</td>\n",
       "      <td>25,600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>convs.1.0.bias</td>\n",
       "      <td>32</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>convs.1.1.weight</td>\n",
       "      <td>32</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>convs.1.1.bias</td>\n",
       "      <td>32</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>dense.0.weight</td>\n",
       "      <td>9,632,000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>dense.0.bias</td>\n",
       "      <td>1,000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>dense.3.weight</td>\n",
       "      <td>1,000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>dense.3.bias</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                name   # params\n",
       "0   convs.0.0.weight     16,000\n",
       "1     convs.0.0.bias         16\n",
       "2   convs.0.1.weight         16\n",
       "3     convs.0.1.bias         16\n",
       "4   convs.1.0.weight     25,600\n",
       "5     convs.1.0.bias         32\n",
       "6   convs.1.1.weight         32\n",
       "7     convs.1.1.bias         32\n",
       "8     dense.0.weight  9,632,000\n",
       "9       dense.0.bias      1,000\n",
       "10    dense.3.weight      1,000\n",
       "11      dense.3.bias          1"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summary, n_params = model.get_summary()\n",
    "print(\"{:,} total parameters\".format(n_params))\n",
    "summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-06T12:32:07.047375Z",
     "start_time": "2019-03-06T12:32:02.524177Z"
    },
    "deletable": false,
    "editable": false,
    "run_control": {
     "frozen": true
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## DEBUG\n",
    "config_changes = dict(\n",
    "    num_epochs = 1,\n",
    "    eval_step = 0.001,\n",
    "    patience = 2,\n",
    "    revert_after_training = True,\n",
    "    clip = 0.5,\n",
    "    lr = 0.01,\n",
    ")\n",
    "model.update_config(config_changes)\n",
    "\n",
    "_ = model.train(train_loader, valid_loader, verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-06T16:23:01.584016Z",
     "start_time": "2019-03-06T16:20:36.545666Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------- EPOCH 1/10 ----------\n",
      "\n",
      "New best!\n",
      "Step: 1571/15710\n",
      "Total steps: 1571\n",
      "Training Loss (smooth): 3.102\n",
      "Validation Loss: 3.805\n",
      "Maximum GPU consumption so far: 0.483 [GB]\n",
      "\n",
      "New best!\n",
      "Step: 3142/15710\n",
      "Total steps: 3142\n",
      "Training Loss (smooth): 3.180\n",
      "Validation Loss: 3.564\n",
      "Maximum GPU consumption so far: 0.483 [GB]\n",
      "\n",
      "Step: 4713/15710\n",
      "Total steps: 4713\n",
      "Training Loss (smooth): 3.079\n",
      "Validation Loss: 3.699\n",
      "Maximum GPU consumption so far: 0.483 [GB]\n",
      "\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-31-597170706180>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate_config\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig_changes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalid_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Preperatory training finished!\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/Kaggle/LANL-Earthquake-prediction/utils/model_wrapper.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, train_loader, val_loader, verbose)\u001b[0m\n\u001b[1;32m    148\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'clip'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m                     nn.utils.clip_grad_norm_(self.net.parameters(), \n\u001b[0;32m--> 150\u001b[0;31m                                              self.config['clip'])\n\u001b[0m\u001b[1;32m    151\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/torch/nn/utils/clip_grad.py\u001b[0m in \u001b[0;36mclip_grad_norm_\u001b[0;34m(parameters, max_norm, norm_type)\u001b[0m\n\u001b[1;32m     30\u001b[0m         \u001b[0mtotal_norm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mp\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mparameters\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 32\u001b[0;31m             \u001b[0mparam_norm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrad\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnorm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnorm_type\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     33\u001b[0m             \u001b[0mtotal_norm\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mparam_norm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m**\u001b[0m \u001b[0mnorm_type\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m         \u001b[0mtotal_norm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtotal_norm\u001b[0m \u001b[0;34m**\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m1.\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mnorm_type\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/torch/tensor.py\u001b[0m in \u001b[0;36mnorm\u001b[0;34m(self, p, dim, keepdim)\u001b[0m\n\u001b[1;32m    250\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mnorm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"fro\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkeepdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    251\u001b[0m         \u001b[0;34mr\"\"\"See :func: `torch.norm`\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 252\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnorm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkeepdim\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    253\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    254\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mbtrifact\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minfo\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpivot\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/torch/functional.py\u001b[0m in \u001b[0;36mnorm\u001b[0;34m(input, p, dim, keepdim, out)\u001b[0m\n\u001b[1;32m    716\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_VariableFunctions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrobenius_norm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    717\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mp\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;34m\"nuc\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 718\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_VariableFunctions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnorm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    719\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    720\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mp\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"fro\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "## Preparatory training with higher learning rate and lower gradient clipping\n",
    "config_changes = dict(\n",
    "    num_epochs = 10,\n",
    "    eval_step = 0.1,\n",
    "    patience = 50,\n",
    "    revert_after_training = True,\n",
    "    clip = 0.5,\n",
    "    lr = 0.01,\n",
    ")\n",
    "model.update_config(config_changes)\n",
    "\n",
    "_ = model.train(train_loader, valid_loader, verbose=2)\n",
    "print(\"Preperatory training finished!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-06T13:00:15.255709Z",
     "start_time": "2019-03-06T12:59:35.253982Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------- EPOCH 1/100 ----------\n",
      "\n",
      "Step: 78/1571\n",
      "Total steps: 78\n",
      "Training Loss (smooth): 3.531\n",
      "Validation Loss: 3.565\n",
      "Maximum GPU consumption so far: 0.698 [GB]\n",
      "\n",
      "Step: 156/1571\n",
      "Total steps: 156\n",
      "Training Loss (smooth): 3.274\n",
      "Validation Loss: 3.665\n",
      "Maximum GPU consumption so far: 0.698 [GB]\n",
      "\n",
      "Step: 234/1571\n",
      "Total steps: 234\n",
      "Training Loss (smooth): 3.209\n",
      "Validation Loss: 3.703\n",
      "Maximum GPU consumption so far: 0.698 [GB]\n",
      "\n",
      "Step: 312/1571\n",
      "Total steps: 312\n",
      "Training Loss (smooth): 3.194\n",
      "Validation Loss: 3.723\n",
      "Maximum GPU consumption so far: 0.698 [GB]\n",
      "\n",
      "Step: 390/1571\n",
      "Total steps: 390\n",
      "Training Loss (smooth): 3.113\n",
      "Validation Loss: 3.722\n",
      "Maximum GPU consumption so far: 0.698 [GB]\n",
      "\n",
      "Step: 468/1571\n",
      "Total steps: 468\n",
      "Training Loss (smooth): 3.075\n",
      "Validation Loss: 3.690\n",
      "Maximum GPU consumption so far: 0.698 [GB]\n",
      "\n",
      "Step: 546/1571\n",
      "Total steps: 546\n",
      "Training Loss (smooth): 3.120\n",
      "Validation Loss: 3.748\n",
      "Maximum GPU consumption so far: 0.698 [GB]\n",
      "\n",
      "Step: 624/1571\n",
      "Total steps: 624\n",
      "Training Loss (smooth): 3.124\n",
      "Validation Loss: 3.748\n",
      "Maximum GPU consumption so far: 0.698 [GB]\n",
      "\n",
      "Step: 702/1571\n",
      "Total steps: 702\n",
      "Training Loss (smooth): 3.102\n",
      "Validation Loss: 3.807\n",
      "Maximum GPU consumption so far: 0.698 [GB]\n",
      "\n",
      "Step: 780/1571\n",
      "Total steps: 780\n",
      "Training Loss (smooth): 3.099\n",
      "Validation Loss: 3.784\n",
      "Maximum GPU consumption so far: 0.698 [GB]\n",
      "\n",
      "Best validation loss: 3.4110096868430597\n",
      "At step: 2\n"
     ]
    }
   ],
   "source": [
    "## Full training\n",
    "config_changes = dict(\n",
    "    num_epochs = 100,\n",
    "    patience = config['patience'],\n",
    "    revert_after_training = True,\n",
    "    clip = config['clip'],\n",
    "    lr = config['lr'],\n",
    ")\n",
    "model.update_config(config_changes)\n",
    "\n",
    "_ = model.train(train_loader, valid_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-06T11:13:57.026297Z",
     "start_time": "2019-03-06T11:10:41.211Z"
    }
   },
   "outputs": [],
   "source": [
    "# model.save_state('checkpoints', 'model0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-06T11:13:57.026903Z",
     "start_time": "2019-03-06T11:10:41.214Z"
    }
   },
   "outputs": [],
   "source": [
    "# model.load_state('checkpoints/model0')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-06T13:10:42.811029Z",
     "start_time": "2019-03-06T13:10:42.794864Z"
    }
   },
   "outputs": [],
   "source": [
    "test_dataset = EarthquakeDatasetTest(test_dir)\n",
    "test_loader = DataLoader(test_dataset,\n",
    "                         batch_size=100, \n",
    "                         shuffle=False, \n",
    "                         num_workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-06T13:11:20.937672Z",
     "start_time": "2019-03-06T13:10:43.921890Z"
    }
   },
   "outputs": [],
   "source": [
    "preds, ids = model.predict(test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-06T13:13:29.592075Z",
     "start_time": "2019-03-06T13:13:29.588462Z"
    }
   },
   "outputs": [],
   "source": [
    "submission = pd.DataFrame({\n",
    "    'seg_id' : ids,\n",
    "    'time_to_failure' : preds,\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-06T13:13:59.667920Z",
     "start_time": "2019-03-06T13:13:59.634027Z"
    }
   },
   "outputs": [],
   "source": [
    "submission.to_csv('../submission.csv', index=None)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "230px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
