{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Ideas\n",
    "- Train a very large CNN-dense network on big computer:\n",
    "    - Use window step size of 1\n",
    "    - Problem: sequences are so long that the model is more likely to overfit than to learn useful things\n",
    "    - Solution: do random masking on data as sort of regularization\n",
    "    - 1D CNNs with smaller stride, followed by just dense should be a decent architecture\n",
    "    - If its not too difficult, do CNN for dim-reduction followed by transformer block\n",
    "- Split a sequence into chunks and do manual feature engineering:\n",
    "    - Pro: Solves the overfitting problem with long sequences\n",
    "    - Pro: trains faster\n",
    "    - Con: Removes one of the main benefits of NNs (automatic feature engineering)\n",
    "    - Con: requires clever and careful feature engineering\n",
    "    - Con: might be more computationally heavy if feat eng is done on the fly\n",
    "- Try transformer/self-attention architecture\n",
    "- Try the feature engineering approach for validation\n",
    "    - Maybe try continuous prediction? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-11T12:57:33.551537Z",
     "start_time": "2019-03-11T12:57:32.674206Z"
    }
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "if not '../' in sys.path:\n",
    "    sys.path.append('../')\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "import importlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-11T12:57:33.554266Z",
     "start_time": "2019-03-11T12:57:33.552575Z"
    }
   },
   "outputs": [],
   "source": [
    "second_earthquake = 50085877\n",
    "test_len = 150000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read in and preprocess data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-11T12:57:33.569417Z",
     "start_time": "2019-03-11T12:57:33.555118Z"
    }
   },
   "outputs": [],
   "source": [
    "data_dir = '../data/'\n",
    "preprocessed_dir = data_dir + 'preprocessed/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-11T12:58:49.334973Z",
     "start_time": "2019-03-11T12:57:33.570389Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aiwizo/anaconda3/lib/python3.6/site-packages/pandas/core/series.py:3724: FutureWarning: from_csv is deprecated. Please use read_csv(...) instead. Note that some of the default arguments are different, so please refer to the documentation for from_csv when changing your function calls\n",
      "  infer_datetime_format=infer_datetime_format)\n"
     ]
    }
   ],
   "source": [
    "train_data = pd.read_csv(data_dir + 'train.csv',  dtype={'acoustic_data': np.float32, 'time_to_failure': np.float32}).values\n",
    "test_dir = data_dir + 'test/'\n",
    "train_desc = pd.Series.from_csv(preprocessed_dir + 'training_data_description.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-11T12:58:51.949036Z",
     "start_time": "2019-03-11T12:58:49.335977Z"
    }
   },
   "outputs": [],
   "source": [
    "## Drop some of the training data for memory efficiency\n",
    "data_frac = 1.0\n",
    "train_data = train_data[:int(data_frac * len(train_data))].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-11T12:58:51.951847Z",
     "start_time": "2019-03-11T12:58:51.949946Z"
    }
   },
   "outputs": [],
   "source": [
    "def scale(acoustic_data, standard=True):\n",
    "    if not standard:\n",
    "        ## Puts values in range [-1, 1]\n",
    "        acoustic_data = 2 * (acoustic_data - train_desc['mean']) / (train_desc['max'] - train_desc['min'])\n",
    "    else:\n",
    "        acoustic_data = (acoustic_data - train_desc['mean']) / train_desc['std']\n",
    "        \n",
    "    return acoustic_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-11T12:58:53.435074Z",
     "start_time": "2019-03-11T12:58:51.953285Z"
    }
   },
   "outputs": [],
   "source": [
    "train_data[:, 0] = scale(train_data[:, 0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-11T13:38:00.519253Z",
     "start_time": "2019-03-11T13:38:00.502743Z"
    }
   },
   "outputs": [],
   "source": [
    "import utils.dataset\n",
    "importlib.reload(utils.dataset)\n",
    "from utils.dataset import *\n",
    "\n",
    "seq_len, n_features = engineer_features(train_data[0:test_len, 0]).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-11T13:38:01.044456Z",
     "start_time": "2019-03-11T13:38:01.018335Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda.\n"
     ]
    }
   ],
   "source": [
    "## Model config\n",
    "chunk_size = 1000\n",
    "config = dict(\n",
    "    \n",
    "    data_dir = data_dir,\n",
    "    use_cuda = torch.cuda.is_available(),\n",
    "    seq_len = seq_len,\n",
    "    n_features = n_features,\n",
    "    \n",
    "    ## Training parameters\n",
    "    batch_size = 32,\n",
    "    lr = 0.001,\n",
    "    num_epochs = 20,\n",
    "    clip = 0.1, # Gradient clipping\n",
    "    eval_step = 0.1, # how often to evaluate, per epoch. E.g., 0.5 -> 2 times per epoch\n",
    "    patience = 10, # patience (in nr of evals) for early stopping. If None, will not use early stopping \n",
    "    revert_after_training = True, # If true, reverts model parameters after training to best found during early stopping\n",
    "    \n",
    "    ## Model hyperparameters\n",
    "    model_choice = 1,\n",
    "    optim_choice = 0,\n",
    "    hidden_size = 48,\n",
    "    dropout = 0.2,\n",
    "    dense_size = 10,\n",
    "    bidirectional = False,\n",
    "    num_layers = 1,\n",
    ")\n",
    "\n",
    "device = torch.device(\"cuda\" if config['use_cuda'] else \"cpu\")\n",
    "print(\"Using {}.\".format(device))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-11T13:38:01.935369Z",
     "start_time": "2019-03-11T13:38:01.932583Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "578,909,602 train examples.\n",
      "332 valid examples.\n"
     ]
    }
   ],
   "source": [
    "# valid_frac = 0.2\n",
    "# n_train_data = int(len(train_data) * (1-valid_frac))\n",
    "\n",
    "X_train = train_data[second_earthquake + 1:]\n",
    "X_valid = train_data[:second_earthquake + 1]\n",
    "\n",
    "train_dataset = EarthquakeDatasetTrain(X_train, window_step=1, mask_prob=0.0)\n",
    "valid_dataset = EarthquakeDatasetTrain(X_valid, window_step=150000)\n",
    "\n",
    "train_loader = RandomLoader(train_dataset, \n",
    "                            batch_size=config['batch_size'],\n",
    "                            num_epoch_steps=1000)\n",
    "\n",
    "valid_loader = DataLoader(valid_dataset, \n",
    "                          batch_size=50, \n",
    "                          shuffle=False)\n",
    "\n",
    "print(\"{:,} train examples.\".format(len(train_dataset)))\n",
    "print(\"{:,} valid examples.\".format(len(valid_dataset)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-11T13:38:02.905480Z",
     "start_time": "2019-03-11T13:38:02.899798Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import utils.models\n",
    "importlib.reload(utils.models)\n",
    "from utils.models import *\n",
    "\n",
    "import utils.model_wrapper\n",
    "importlib.reload(utils.model_wrapper)\n",
    "from utils.model_wrapper import *\n",
    "model = ModelWrapper(config=config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-11T13:38:03.650903Z",
     "start_time": "2019-03-11T13:38:03.618885Z"
    },
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9,429 total parameters\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th># params</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>rnn.weight_ih_l0</td>\n",
       "      <td>1,728</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>rnn.weight_hh_l0</td>\n",
       "      <td>6,912</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>rnn.bias_ih_l0</td>\n",
       "      <td>144</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>rnn.bias_hh_l0</td>\n",
       "      <td>144</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>dense.weight</td>\n",
       "      <td>480</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>dense.bias</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>classifier.weight</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>classifier.bias</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                name # params\n",
       "0   rnn.weight_ih_l0    1,728\n",
       "1   rnn.weight_hh_l0    6,912\n",
       "2     rnn.bias_ih_l0      144\n",
       "3     rnn.bias_hh_l0      144\n",
       "4       dense.weight      480\n",
       "5         dense.bias       10\n",
       "6  classifier.weight       10\n",
       "7    classifier.bias        1"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summary, n_params = model.get_summary()\n",
    "print(\"{:,} total parameters\".format(n_params))\n",
    "summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-08T13:47:54.285829Z",
     "start_time": "2019-03-08T13:47:50.373978Z"
    },
    "deletable": false,
    "editable": false,
    "run_control": {
     "frozen": true
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## DEBUG\n",
    "for batch in train_loader:\n",
    "    break\n",
    "\n",
    "output = model.net.forward(batch['features'].to(device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-11T15:18:04.293102Z",
     "start_time": "2019-03-11T13:38:06.086577Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------- EPOCH 1/20 ----------\n",
      "\n",
      "New best!\n",
      "Step: 200/1000\n",
      "Total steps: 200\n",
      "Training Loss (smooth): 4.231\n",
      "Validation Loss: 3.124\n",
      "Maximum GPU consumption so far: 0.031 [GB]\n",
      "\n",
      "New best!\n",
      "Step: 400/1000\n",
      "Total steps: 400\n",
      "Training Loss (smooth): 3.488\n",
      "Validation Loss: 3.081\n",
      "Maximum GPU consumption so far: 0.031 [GB]\n",
      "\n",
      "New best!\n",
      "Step: 600/1000\n",
      "Total steps: 600\n",
      "Training Loss (smooth): 3.326\n",
      "Validation Loss: 3.079\n",
      "Maximum GPU consumption so far: 0.031 [GB]\n",
      "\n",
      "Step: 800/1000\n",
      "Total steps: 800\n",
      "Training Loss (smooth): 3.354\n",
      "Validation Loss: 3.087\n",
      "Maximum GPU consumption so far: 0.031 [GB]\n",
      "\n",
      "Step: 1000/1000\n",
      "Total steps: 1000\n",
      "Training Loss (smooth): 3.331\n",
      "Validation Loss: 3.094\n",
      "Maximum GPU consumption so far: 0.031 [GB]\n",
      "\n",
      "---------- EPOCH 2/20 ----------\n",
      "\n",
      "Step: 200/1000\n",
      "Total steps: 1200\n",
      "Training Loss (smooth): 3.262\n",
      "Validation Loss: 3.085\n",
      "Maximum GPU consumption so far: 0.031 [GB]\n",
      "\n",
      "Step: 400/1000\n",
      "Total steps: 1400\n",
      "Training Loss (smooth): 3.250\n",
      "Validation Loss: 3.090\n",
      "Maximum GPU consumption so far: 0.031 [GB]\n",
      "\n",
      "Step: 600/1000\n",
      "Total steps: 1600\n",
      "Training Loss (smooth): 3.263\n",
      "Validation Loss: 3.084\n",
      "Maximum GPU consumption so far: 0.031 [GB]\n",
      "\n",
      "Step: 800/1000\n",
      "Total steps: 1800\n",
      "Training Loss (smooth): 3.207\n",
      "Validation Loss: 3.082\n",
      "Maximum GPU consumption so far: 0.031 [GB]\n",
      "\n",
      "Step: 1000/1000\n",
      "Total steps: 2000\n",
      "Training Loss (smooth): 3.254\n",
      "Validation Loss: 3.080\n",
      "Maximum GPU consumption so far: 0.031 [GB]\n",
      "\n",
      "---------- EPOCH 3/20 ----------\n",
      "\n",
      "Step: 200/1000\n",
      "Total steps: 2200\n",
      "Training Loss (smooth): 3.235\n",
      "Validation Loss: 3.081\n",
      "Maximum GPU consumption so far: 0.031 [GB]\n",
      "\n",
      "New best!\n",
      "Step: 400/1000\n",
      "Total steps: 2400\n",
      "Training Loss (smooth): 3.230\n",
      "Validation Loss: 3.076\n",
      "Maximum GPU consumption so far: 0.031 [GB]\n",
      "\n",
      "New best!\n",
      "Step: 600/1000\n",
      "Total steps: 2600\n",
      "Training Loss (smooth): 3.225\n",
      "Validation Loss: 3.071\n",
      "Maximum GPU consumption so far: 0.031 [GB]\n",
      "\n",
      "New best!\n",
      "Step: 800/1000\n",
      "Total steps: 2800\n",
      "Training Loss (smooth): 3.199\n",
      "Validation Loss: 3.070\n",
      "Maximum GPU consumption so far: 0.031 [GB]\n",
      "\n",
      "New best!\n",
      "Step: 1000/1000\n",
      "Total steps: 3000\n",
      "Training Loss (smooth): 3.226\n",
      "Validation Loss: 3.070\n",
      "Maximum GPU consumption so far: 0.031 [GB]\n",
      "\n",
      "---------- EPOCH 4/20 ----------\n",
      "\n",
      "Step: 200/1000\n",
      "Total steps: 3200\n",
      "Training Loss (smooth): 3.262\n",
      "Validation Loss: 3.070\n",
      "Maximum GPU consumption so far: 0.031 [GB]\n",
      "\n",
      "Step: 400/1000\n",
      "Total steps: 3400\n",
      "Training Loss (smooth): 3.204\n",
      "Validation Loss: 3.075\n",
      "Maximum GPU consumption so far: 0.031 [GB]\n",
      "\n",
      "New best!\n",
      "Step: 600/1000\n",
      "Total steps: 3600\n",
      "Training Loss (smooth): 3.271\n",
      "Validation Loss: 3.069\n",
      "Maximum GPU consumption so far: 0.031 [GB]\n",
      "\n",
      "New best!\n",
      "Step: 800/1000\n",
      "Total steps: 3800\n",
      "Training Loss (smooth): 3.238\n",
      "Validation Loss: 3.064\n",
      "Maximum GPU consumption so far: 0.031 [GB]\n",
      "\n",
      "New best!\n",
      "Step: 1000/1000\n",
      "Total steps: 4000\n",
      "Training Loss (smooth): 3.253\n",
      "Validation Loss: 3.064\n",
      "Maximum GPU consumption so far: 0.031 [GB]\n",
      "\n",
      "---------- EPOCH 5/20 ----------\n",
      "\n",
      "New best!\n",
      "Step: 200/1000\n",
      "Total steps: 4200\n",
      "Training Loss (smooth): 3.184\n",
      "Validation Loss: 3.062\n",
      "Maximum GPU consumption so far: 0.031 [GB]\n",
      "\n",
      "Step: 400/1000\n",
      "Total steps: 4400\n",
      "Training Loss (smooth): 3.182\n",
      "Validation Loss: 3.063\n",
      "Maximum GPU consumption so far: 0.031 [GB]\n",
      "\n",
      "Step: 600/1000\n",
      "Total steps: 4600\n",
      "Training Loss (smooth): 3.214\n",
      "Validation Loss: 3.064\n",
      "Maximum GPU consumption so far: 0.031 [GB]\n",
      "\n",
      "Step: 800/1000\n",
      "Total steps: 4800\n",
      "Training Loss (smooth): 3.192\n",
      "Validation Loss: 3.066\n",
      "Maximum GPU consumption so far: 0.031 [GB]\n",
      "\n",
      "Step: 1000/1000\n",
      "Total steps: 5000\n",
      "Training Loss (smooth): 3.179\n",
      "Validation Loss: 3.063\n",
      "Maximum GPU consumption so far: 0.031 [GB]\n",
      "\n",
      "---------- EPOCH 6/20 ----------\n",
      "\n",
      "New best!\n",
      "Step: 200/1000\n",
      "Total steps: 5200\n",
      "Training Loss (smooth): 3.161\n",
      "Validation Loss: 3.062\n",
      "Maximum GPU consumption so far: 0.031 [GB]\n",
      "\n",
      "Step: 400/1000\n",
      "Total steps: 5400\n",
      "Training Loss (smooth): 3.186\n",
      "Validation Loss: 3.063\n",
      "Maximum GPU consumption so far: 0.031 [GB]\n",
      "\n",
      "New best!\n",
      "Step: 600/1000\n",
      "Total steps: 5600\n",
      "Training Loss (smooth): 3.182\n",
      "Validation Loss: 3.061\n",
      "Maximum GPU consumption so far: 0.031 [GB]\n",
      "\n",
      "New best!\n",
      "Step: 800/1000\n",
      "Total steps: 5800\n",
      "Training Loss (smooth): 3.190\n",
      "Validation Loss: 3.060\n",
      "Maximum GPU consumption so far: 0.031 [GB]\n",
      "\n",
      "New best!\n",
      "Step: 1000/1000\n",
      "Total steps: 6000\n",
      "Training Loss (smooth): 3.198\n",
      "Validation Loss: 3.060\n",
      "Maximum GPU consumption so far: 0.031 [GB]\n",
      "\n",
      "---------- EPOCH 7/20 ----------\n",
      "\n",
      "Step: 200/1000\n",
      "Total steps: 6200\n",
      "Training Loss (smooth): 3.225\n",
      "Validation Loss: 3.061\n",
      "Maximum GPU consumption so far: 0.031 [GB]\n",
      "\n",
      "New best!\n",
      "Step: 400/1000\n",
      "Total steps: 6400\n",
      "Training Loss (smooth): 3.209\n",
      "Validation Loss: 3.057\n",
      "Maximum GPU consumption so far: 0.031 [GB]\n",
      "\n",
      "Step: 600/1000\n",
      "Total steps: 6600\n",
      "Training Loss (smooth): 3.218\n",
      "Validation Loss: 3.059\n",
      "Maximum GPU consumption so far: 0.031 [GB]\n",
      "\n",
      "Step: 800/1000\n",
      "Total steps: 6800\n",
      "Training Loss (smooth): 3.154\n",
      "Validation Loss: 3.062\n",
      "Maximum GPU consumption so far: 0.031 [GB]\n",
      "\n",
      "Step: 1000/1000\n",
      "Total steps: 7000\n",
      "Training Loss (smooth): 3.138\n",
      "Validation Loss: 3.058\n",
      "Maximum GPU consumption so far: 0.031 [GB]\n",
      "\n",
      "---------- EPOCH 8/20 ----------\n",
      "\n",
      "Step: 200/1000\n",
      "Total steps: 7200\n",
      "Training Loss (smooth): 3.138\n",
      "Validation Loss: 3.058\n",
      "Maximum GPU consumption so far: 0.031 [GB]\n",
      "\n",
      "Step: 400/1000\n",
      "Total steps: 7400\n",
      "Training Loss (smooth): 3.155\n",
      "Validation Loss: 3.057\n",
      "Maximum GPU consumption so far: 0.031 [GB]\n",
      "\n",
      "Step: 600/1000\n",
      "Total steps: 7600\n",
      "Training Loss (smooth): 3.147\n",
      "Validation Loss: 3.060\n",
      "Maximum GPU consumption so far: 0.031 [GB]\n",
      "\n",
      "Step: 800/1000\n",
      "Total steps: 7800\n",
      "Training Loss (smooth): 3.203\n",
      "Validation Loss: 3.061\n",
      "Maximum GPU consumption so far: 0.031 [GB]\n",
      "\n",
      "New best!\n",
      "Step: 1000/1000\n",
      "Total steps: 8000\n",
      "Training Loss (smooth): 3.188\n",
      "Validation Loss: 3.056\n",
      "Maximum GPU consumption so far: 0.031 [GB]\n",
      "\n",
      "---------- EPOCH 9/20 ----------\n",
      "\n",
      "Step: 200/1000\n",
      "Total steps: 8200\n",
      "Training Loss (smooth): 3.172\n",
      "Validation Loss: 3.057\n",
      "Maximum GPU consumption so far: 0.031 [GB]\n",
      "\n",
      "Step: 400/1000\n",
      "Total steps: 8400\n",
      "Training Loss (smooth): 3.120\n",
      "Validation Loss: 3.059\n",
      "Maximum GPU consumption so far: 0.031 [GB]\n",
      "\n",
      "Step: 600/1000\n",
      "Total steps: 8600\n",
      "Training Loss (smooth): 3.114\n",
      "Validation Loss: 3.057\n",
      "Maximum GPU consumption so far: 0.031 [GB]\n",
      "\n",
      "Step: 800/1000\n",
      "Total steps: 8800\n",
      "Training Loss (smooth): 3.165\n",
      "Validation Loss: 3.056\n",
      "Maximum GPU consumption so far: 0.031 [GB]\n",
      "\n",
      "Step: 1000/1000\n",
      "Total steps: 9000\n",
      "Training Loss (smooth): 3.139\n",
      "Validation Loss: 3.058\n",
      "Maximum GPU consumption so far: 0.031 [GB]\n",
      "\n",
      "---------- EPOCH 10/20 ----------\n",
      "\n",
      "Step: 200/1000\n",
      "Total steps: 9200\n",
      "Training Loss (smooth): 3.125\n",
      "Validation Loss: 3.059\n",
      "Maximum GPU consumption so far: 0.031 [GB]\n",
      "\n",
      "Step: 400/1000\n",
      "Total steps: 9400\n",
      "Training Loss (smooth): 3.146\n",
      "Validation Loss: 3.058\n",
      "Maximum GPU consumption so far: 0.031 [GB]\n",
      "\n",
      "Step: 600/1000\n",
      "Total steps: 9600\n",
      "Training Loss (smooth): 3.140\n",
      "Validation Loss: 3.059\n",
      "Maximum GPU consumption so far: 0.031 [GB]\n",
      "\n",
      "Step: 800/1000\n",
      "Total steps: 9800\n",
      "Training Loss (smooth): 3.145\n",
      "Validation Loss: 3.059\n",
      "Maximum GPU consumption so far: 0.031 [GB]\n",
      "\n",
      "Step: 1000/1000\n",
      "Total steps: 10000\n",
      "Training Loss (smooth): 3.138\n",
      "Validation Loss: 3.061\n",
      "Maximum GPU consumption so far: 0.031 [GB]\n",
      "\n",
      "---------- EPOCH 11/20 ----------\n",
      "\n",
      "Step: 200/1000\n",
      "Total steps: 10200\n",
      "Training Loss (smooth): 3.110\n",
      "Validation Loss: 3.060\n",
      "Maximum GPU consumption so far: 0.031 [GB]\n",
      "\n",
      "Step: 400/1000\n",
      "Total steps: 10400\n",
      "Training Loss (smooth): 3.120\n",
      "Validation Loss: 3.057\n",
      "Maximum GPU consumption so far: 0.031 [GB]\n",
      "\n",
      "Step: 600/1000\n",
      "Total steps: 10600\n",
      "Training Loss (smooth): 3.089\n",
      "Validation Loss: 3.057\n",
      "Maximum GPU consumption so far: 0.031 [GB]\n",
      "\n",
      "Step: 800/1000\n",
      "Total steps: 10800\n",
      "Training Loss (smooth): 3.134\n",
      "Validation Loss: 3.062\n",
      "Maximum GPU consumption so far: 0.031 [GB]\n",
      "\n",
      "Step: 1000/1000\n",
      "Total steps: 11000\n",
      "Training Loss (smooth): 3.161\n",
      "Validation Loss: 3.060\n",
      "Maximum GPU consumption so far: 0.031 [GB]\n",
      "\n",
      "---------- EPOCH 12/20 ----------\n",
      "\n",
      "Step: 200/1000\n",
      "Total steps: 11200\n",
      "Training Loss (smooth): 3.113\n",
      "Validation Loss: 3.057\n",
      "Maximum GPU consumption so far: 0.031 [GB]\n",
      "\n",
      "Step: 400/1000\n",
      "Total steps: 11400\n",
      "Training Loss (smooth): 3.157\n",
      "Validation Loss: 3.057\n",
      "Maximum GPU consumption so far: 0.031 [GB]\n",
      "\n",
      "Step: 600/1000\n",
      "Total steps: 11600\n",
      "Training Loss (smooth): 3.172\n",
      "Validation Loss: 3.059\n",
      "Maximum GPU consumption so far: 0.031 [GB]\n",
      "\n",
      "Step: 800/1000\n",
      "Total steps: 11800\n",
      "Training Loss (smooth): 3.126\n",
      "Validation Loss: 3.058\n",
      "Maximum GPU consumption so far: 0.031 [GB]\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 1000/1000\n",
      "Total steps: 12000\n",
      "Training Loss (smooth): 3.105\n",
      "Validation Loss: 3.057\n",
      "Maximum GPU consumption so far: 0.031 [GB]\n",
      "\n",
      "---------- EPOCH 13/20 ----------\n",
      "\n",
      "Best validation loss: 3.0559898374310457\n",
      "At step: 8001\n",
      "Preperatory training finished!\n"
     ]
    }
   ],
   "source": [
    "## Preparatory training with higher learning rate and lower gradient clipping\n",
    "config_changes = dict(\n",
    "    num_epochs = 20,\n",
    "    eval_step = 0.2,\n",
    "    patience = 20,\n",
    "    revert_after_training = True,\n",
    "    clip = None,\n",
    "    lr = 0.0005,\n",
    ")\n",
    "model.update_config(config_changes)\n",
    "\n",
    "_ = model.train(train_loader, valid_loader, verbose=2)\n",
    "print(\"Preperatory training finished!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## Full training\n",
    "config_changes = dict(\n",
    "    num_epochs = 100,\n",
    "    patience = config['patience'],\n",
    "    revert_after_training = True,\n",
    "    clip = config['clip'],\n",
    "    lr = config['lr'],\n",
    ")\n",
    "model.update_config(config_changes)\n",
    "\n",
    "_ = model.train(train_loader, valid_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-08T10:12:44.346245Z",
     "start_time": "2019-03-08T10:12:44.333999Z"
    }
   },
   "outputs": [],
   "source": [
    "#model.save_state('../checkpoints/', 'model0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-08T10:16:18.972995Z",
     "start_time": "2019-03-08T10:16:18.966331Z"
    }
   },
   "outputs": [],
   "source": [
    "#model = ModelWrapper(pretrained_path='../checkpoints/model0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-08T14:51:35.317990Z",
     "start_time": "2019-03-08T14:51:34.475369Z"
    }
   },
   "outputs": [],
   "source": [
    "preds = model.predict(valid_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-08T14:51:35.374314Z",
     "start_time": "2019-03-08T14:51:35.371596Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5.2348948"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-08T14:51:35.418951Z",
     "start_time": "2019-03-08T14:51:35.416866Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.031452578"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds.std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-08T14:52:48.215973Z",
     "start_time": "2019-03-08T14:52:48.015752Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5.7194834"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train[:, 1].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-08T14:52:36.236546Z",
     "start_time": "2019-03-08T14:52:36.191352Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5.2020006"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_valid[:, 1].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-08T14:52:38.591460Z",
     "start_time": "2019-03-08T14:52:38.453778Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3.5223672"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_valid[:, 1].std()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-08T09:12:50.611492Z",
     "start_time": "2019-03-08T09:12:50.605023Z"
    }
   },
   "outputs": [],
   "source": [
    "test_dataset = EarthquakeDatasetTest(test_dir)\n",
    "test_loader = DataLoader(test_dataset,\n",
    "                         batch_size=100, \n",
    "                         shuffle=False, \n",
    "                         num_workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-08T09:13:12.359272Z",
     "start_time": "2019-03-08T09:12:50.639849Z"
    }
   },
   "outputs": [],
   "source": [
    "preds, ids = model.predict(test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-08T09:13:12.386814Z",
     "start_time": "2019-03-08T09:13:12.384951Z"
    }
   },
   "outputs": [],
   "source": [
    "submission = pd.DataFrame({\n",
    "    'seg_id' : ids,\n",
    "    'time_to_failure' : preds,\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-08T09:13:12.429427Z",
     "start_time": "2019-03-08T09:13:12.423600Z"
    }
   },
   "outputs": [],
   "source": [
    "submission.to_csv('../submission.csv', index=None)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "230px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
