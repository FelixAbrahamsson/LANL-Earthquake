{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Ideas\n",
    "- Train a very large CNN-dense network on big computer:\n",
    "    - Use window step size of 1\n",
    "    - Problem: sequences are so long that the model is more likely to overfit than to learn useful things\n",
    "    - Solution: do random masking on data as sort of regularization\n",
    "    - 1D CNNs with smaller stride, followed by just dense should be a decent architecture\n",
    "    - If its not too difficult, do CNN for dim-reduction followed by transformer block\n",
    "- Split a sequence into chunks and do manual feature engineering:\n",
    "    - Pro: Solves the overfitting problem with long sequences\n",
    "    - Pro: trains faster\n",
    "    - Con: Removes one of the main benefits of NNs (automatic feature engineering)\n",
    "    - Con: requires clever and careful feature engineering\n",
    "    - Con: might be more computationally heavy if feat eng is done on the fly\n",
    "- Try transformer/self-attention architecture\n",
    "- Try the feature engineering approach for validation\n",
    "    - Maybe try continuous prediction? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-08T13:00:38.987583Z",
     "start_time": "2019-03-08T13:00:38.451822Z"
    }
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "if not '../' in sys.path:\n",
    "    sys.path.append('../')\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "import importlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "second_earthquake = 50085877"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read in and preprocess data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-08T13:00:39.538308Z",
     "start_time": "2019-03-08T13:00:39.532357Z"
    }
   },
   "outputs": [],
   "source": [
    "data_dir = '../data/'\n",
    "preprocessed_dir = data_dir + 'preprocessed/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-08T13:02:17.511156Z",
     "start_time": "2019-03-08T13:00:40.207597Z"
    }
   },
   "outputs": [],
   "source": [
    "train_data = pd.read_csv(data_dir + 'train.csv',  dtype={'acoustic_data': np.float32, 'time_to_failure': np.float32}).values\n",
    "test_dir = data_dir + 'test/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-08T13:02:19.443012Z",
     "start_time": "2019-03-08T13:02:17.512117Z"
    }
   },
   "outputs": [],
   "source": [
    "## Drop some of the training data for memory efficiency\n",
    "data_frac = 0.5\n",
    "train_data = train_data[:int(data_frac * len(train_data))].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-08T13:02:19.468389Z",
     "start_time": "2019-03-08T13:02:19.444079Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/felix/anaconda3/lib/python3.6/site-packages/pandas/core/series.py:3727: FutureWarning: from_csv is deprecated. Please use read_csv(...) instead. Note that some of the default arguments are different, so please refer to the documentation for from_csv when changing your function calls\n",
      "  infer_datetime_format=infer_datetime_format)\n"
     ]
    }
   ],
   "source": [
    "train_desc = pd.Series.from_csv(preprocessed_dir + 'training_data_description.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-08T13:46:48.936811Z",
     "start_time": "2019-03-08T13:46:48.928891Z"
    }
   },
   "outputs": [],
   "source": [
    "def scale(acoustic_data, standard=True):\n",
    "    if not standard:\n",
    "        ## Puts values in range [-1, 1]\n",
    "        acoustic_data = 2 * (acoustic_data - train_desc['mean']) / (train_desc['max'] - train_desc['min'])\n",
    "    else:\n",
    "        acoustic_data = (acoustic_data - train_desc['mean']) / train_desc['std']\n",
    "        \n",
    "    return acoustic_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-08T13:46:53.098337Z",
     "start_time": "2019-03-08T13:46:51.574194Z"
    }
   },
   "outputs": [],
   "source": [
    "train_data[:, 0] = scale(train_data[:, 0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-08T13:18:15.537838Z",
     "start_time": "2019-03-08T13:18:15.532770Z"
    }
   },
   "outputs": [],
   "source": [
    "from utils.dataset import FeatureEngineer as FeatureEngineer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-08T13:47:24.637608Z",
     "start_time": "2019-03-08T13:47:24.621744Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda.\n"
     ]
    }
   ],
   "source": [
    "## Model config\n",
    "config = dict(\n",
    "    \n",
    "    data_dir = data_dir,\n",
    "    use_cuda = torch.cuda.is_available(),\n",
    "    seq_len = 150000 // FeatureEngineer.chunk_size,\n",
    "    n_features = FeatureEngineer.n_features,\n",
    "    \n",
    "    ## Training parameters\n",
    "    batch_size = 16,\n",
    "    lr = 0.001,\n",
    "    num_epochs = 20,\n",
    "    clip = 0.1, # Gradient clipping\n",
    "    eval_step = 0.1, # how often to evaluate, per epoch. E.g., 0.5 -> 2 times per epoch\n",
    "    patience = 10, # patience (in nr of evals) for early stopping. If None, will not use early stopping \n",
    "    revert_after_training = True, # If true, reverts model parameters after training to best found during early stopping\n",
    "    \n",
    "    ## Model hyperparameters\n",
    "    model_choice = 1,\n",
    "    optim_choice = 0,\n",
    "    hidden_size = 512,\n",
    "    dropout = 0.2,\n",
    "    dense_size = 1000,\n",
    "    bidirectional = True,\n",
    "    num_layers = 2,\n",
    ")\n",
    "\n",
    "device = torch.device(\"cuda\" if config['use_cuda'] else \"cpu\")\n",
    "print(\"Using {}.\".format(device))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-08T13:36:48.185552Z",
     "start_time": "2019-03-08T13:36:48.179659Z"
    }
   },
   "outputs": [],
   "source": [
    "import utils.dataset\n",
    "importlib.reload(utils.dataset)\n",
    "from utils.dataset import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-08T13:37:45.817077Z",
     "start_time": "2019-03-08T13:37:45.789897Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "264,336 train examples.\n",
      "332 valid examples.\n"
     ]
    }
   ],
   "source": [
    "# valid_frac = 0.2\n",
    "# n_train_data = int(len(train_data) * (1-valid_frac))\n",
    "\n",
    "X_train = train_data[second_earthquake + 1:]\n",
    "X_valid = train_data[:second_earthquake + 1]\n",
    "\n",
    "train_dataset = EarthquakeDatasetTrain(X_train, window_step=1000, mask_prob=0.1)\n",
    "valid_dataset = EarthquakeDatasetTrain(X_valid, window_step=150000)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, \n",
    "                          batch_size=config['batch_size'], \n",
    "                          shuffle=True, \n",
    "                          num_workers=4)\n",
    "\n",
    "valid_loader = DataLoader(valid_dataset, \n",
    "                         batch_size=100, \n",
    "                         shuffle=False, \n",
    "                         num_workers=4)\n",
    "\n",
    "print(\"{:,} train examples.\".format(len(train_dataset)))\n",
    "print(\"{:,} valid examples.\".format(len(valid_dataset)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-08T13:47:48.326506Z",
     "start_time": "2019-03-08T13:47:48.224543Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import utils.models\n",
    "importlib.reload(utils.models)\n",
    "from utils.models import *\n",
    "\n",
    "import utils.model_wrapper\n",
    "importlib.reload(utils.model_wrapper)\n",
    "from utils.model_wrapper import *\n",
    "model = ModelWrapper(config=config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-08T13:47:49.035348Z",
     "start_time": "2019-03-08T13:47:49.005051Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9,484,241 total parameters\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th># params</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>rnn.weight_ih_l0</td>\n",
       "      <td>26,624</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>rnn.weight_hh_l0</td>\n",
       "      <td>1,048,576</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>rnn.bias_ih_l0</td>\n",
       "      <td>2,048</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>rnn.bias_hh_l0</td>\n",
       "      <td>2,048</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>rnn.weight_ih_l0_reverse</td>\n",
       "      <td>26,624</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>rnn.weight_hh_l0_reverse</td>\n",
       "      <td>1,048,576</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>rnn.bias_ih_l0_reverse</td>\n",
       "      <td>2,048</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>rnn.bias_hh_l0_reverse</td>\n",
       "      <td>2,048</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>rnn.weight_ih_l1</td>\n",
       "      <td>2,097,152</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>rnn.weight_hh_l1</td>\n",
       "      <td>1,048,576</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>rnn.bias_ih_l1</td>\n",
       "      <td>2,048</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>rnn.bias_hh_l1</td>\n",
       "      <td>2,048</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>rnn.weight_ih_l1_reverse</td>\n",
       "      <td>2,097,152</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>rnn.weight_hh_l1_reverse</td>\n",
       "      <td>1,048,576</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>rnn.bias_ih_l1_reverse</td>\n",
       "      <td>2,048</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>rnn.bias_hh_l1_reverse</td>\n",
       "      <td>2,048</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>dense.weight</td>\n",
       "      <td>1,024,000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>dense.bias</td>\n",
       "      <td>1,000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>classifier.weight</td>\n",
       "      <td>1,000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>classifier.bias</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                        name   # params\n",
       "0           rnn.weight_ih_l0     26,624\n",
       "1           rnn.weight_hh_l0  1,048,576\n",
       "2             rnn.bias_ih_l0      2,048\n",
       "3             rnn.bias_hh_l0      2,048\n",
       "4   rnn.weight_ih_l0_reverse     26,624\n",
       "5   rnn.weight_hh_l0_reverse  1,048,576\n",
       "6     rnn.bias_ih_l0_reverse      2,048\n",
       "7     rnn.bias_hh_l0_reverse      2,048\n",
       "8           rnn.weight_ih_l1  2,097,152\n",
       "9           rnn.weight_hh_l1  1,048,576\n",
       "10            rnn.bias_ih_l1      2,048\n",
       "11            rnn.bias_hh_l1      2,048\n",
       "12  rnn.weight_ih_l1_reverse  2,097,152\n",
       "13  rnn.weight_hh_l1_reverse  1,048,576\n",
       "14    rnn.bias_ih_l1_reverse      2,048\n",
       "15    rnn.bias_hh_l1_reverse      2,048\n",
       "16              dense.weight  1,024,000\n",
       "17                dense.bias      1,000\n",
       "18         classifier.weight      1,000\n",
       "19           classifier.bias          1"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summary, n_params = model.get_summary()\n",
    "print(\"{:,} total parameters\".format(n_params))\n",
    "summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-08T13:47:54.285829Z",
     "start_time": "2019-03-08T13:47:50.373978Z"
    },
    "deletable": false,
    "editable": false,
    "run_control": {
     "frozen": true
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## DEBUG\n",
    "for batch in train_loader:\n",
    "    break\n",
    "\n",
    "output = model.net.forward(batch['features'].to(device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-08T13:44:34.470253Z",
     "start_time": "2019-03-08T13:37:56.623988Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## Preparatory training with higher learning rate and lower gradient clipping\n",
    "config_changes = dict(\n",
    "    num_epochs = 20,\n",
    "    eval_step = 0.01,\n",
    "    patience = 40,\n",
    "    revert_after_training = True,\n",
    "    clip = 0.1,\n",
    "    lr = 0.001,\n",
    ")\n",
    "model.update_config(config_changes)\n",
    "\n",
    "_ = model.train(train_loader, valid_loader, verbose=2)\n",
    "print(\"Preperatory training finished!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## Full training\n",
    "config_changes = dict(\n",
    "    num_epochs = 100,\n",
    "    patience = config['patience'],\n",
    "    revert_after_training = True,\n",
    "    clip = config['clip'],\n",
    "    lr = config['lr'],\n",
    ")\n",
    "model.update_config(config_changes)\n",
    "\n",
    "_ = model.train(train_loader, valid_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-08T10:12:44.346245Z",
     "start_time": "2019-03-08T10:12:44.333999Z"
    }
   },
   "outputs": [],
   "source": [
    "#model.save_state('../checkpoints/', 'model0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-08T10:16:18.972995Z",
     "start_time": "2019-03-08T10:16:18.966331Z"
    }
   },
   "outputs": [],
   "source": [
    "#model = ModelWrapper(pretrained_path='../checkpoints/model0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-08T13:44:59.078894Z",
     "start_time": "2019-03-08T13:44:47.264320Z"
    }
   },
   "outputs": [],
   "source": [
    "preds = model.predict(valid_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-08T13:44:59.083866Z",
     "start_time": "2019-03-08T13:44:59.080317Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6.069175"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-08T13:44:59.095402Z",
     "start_time": "2019-03-08T13:44:59.085121Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.033578407"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds.std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-08T13:47:04.240528Z",
     "start_time": "2019-03-08T13:47:04.081692Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0044764853"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train[:, 0].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-08T13:47:06.113783Z",
     "start_time": "2019-03-08T13:47:05.230814Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9482863"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train[:, 0].std()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-08T09:12:50.611492Z",
     "start_time": "2019-03-08T09:12:50.605023Z"
    }
   },
   "outputs": [],
   "source": [
    "test_dataset = EarthquakeDatasetTest(test_dir)\n",
    "test_loader = DataLoader(test_dataset,\n",
    "                         batch_size=100, \n",
    "                         shuffle=False, \n",
    "                         num_workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-08T09:13:12.359272Z",
     "start_time": "2019-03-08T09:12:50.639849Z"
    }
   },
   "outputs": [],
   "source": [
    "preds, ids = model.predict(test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-08T09:13:12.386814Z",
     "start_time": "2019-03-08T09:13:12.384951Z"
    }
   },
   "outputs": [],
   "source": [
    "submission = pd.DataFrame({\n",
    "    'seg_id' : ids,\n",
    "    'time_to_failure' : preds,\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-08T09:13:12.429427Z",
     "start_time": "2019-03-08T09:13:12.423600Z"
    }
   },
   "outputs": [],
   "source": [
    "submission.to_csv('../submission.csv', index=None)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "230px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
