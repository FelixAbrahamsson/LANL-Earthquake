{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports and constants "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-05T06:23:04.047275Z",
     "start_time": "2019-04-05T06:23:03.443588Z"
    }
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "if '../' not in sys.path:\n",
    "    sys.path.append('../')\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy import stats\n",
    "import pickle\n",
    "import os\n",
    "import lightgbm as lgb\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import KFold, train_test_split\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "import glob\n",
    "import gc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-05T06:23:04.050109Z",
     "start_time": "2019-04-05T06:23:04.048274Z"
    }
   },
   "outputs": [],
   "source": [
    "data_dir = '../data/'\n",
    "outdir = data_dir + 'non_dl_processed/'\n",
    "if not os.path.exists(outdir):\n",
    "    os.makedirs(outdir)\n",
    "    \n",
    "second_earthquake = 50085877\n",
    "test_len = 150000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The second earthquake happens at 50085877, so we can use the data before that (~8%) as validation data to evaluate different models with."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-04T14:10:39.599362Z",
     "start_time": "2019-04-04T14:07:44.439948Z"
    },
    "deletable": false,
    "editable": false,
    "run_control": {
     "frozen": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(629145480, 2)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train = pd.read_csv(data_dir + 'train.csv')\n",
    "train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-04T14:19:32.829043Z",
     "start_time": "2019-04-04T14:19:32.793521Z"
    },
    "deletable": false,
    "editable": false,
    "run_control": {
     "frozen": true
    }
   },
   "outputs": [],
   "source": [
    "valid = train[:second_earthquake + 1]\n",
    "train = train[second_earthquake + 1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-04T14:38:11.233402Z",
     "start_time": "2019-04-04T14:20:08.680878Z"
    },
    "deletable": false,
    "editable": false,
    "run_control": {
     "frozen": true
    }
   },
   "outputs": [],
   "source": [
    "valid.to_csv(outdir + 'valid.csv', index=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-04T14:39:44.427730Z",
     "start_time": "2019-04-04T14:38:11.256268Z"
    },
    "deletable": false,
    "editable": false,
    "run_control": {
     "frozen": true
    }
   },
   "outputs": [],
   "source": [
    "train.to_csv(outdir + 'train.csv', index=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-04T14:43:28.201567Z",
     "start_time": "2019-04-04T14:43:28.166160Z"
    }
   },
   "outputs": [],
   "source": [
    "def make_chunk_features(seq, chunk_size=1000):\n",
    "\n",
    "    n_chunks = len(seq) // chunk_size\n",
    "    features = []\n",
    "\n",
    "    for i in range(n_chunks):\n",
    "\n",
    "        chunk = seq[i * chunk_size : (i+1) * chunk_size]\n",
    "\n",
    "        chunk_features = process_chunk(chunk)\n",
    "        chunk_features.extend(process_chunk(chunk[-100:]))\n",
    "        chunk_features.extend(process_chunk(chunk[-10:]))\n",
    "        \n",
    "        features.append(chunk_features)\n",
    "\n",
    "    return np.array(features)\n",
    "\n",
    "\n",
    "def process_chunk(chunk):\n",
    "\n",
    "        mean = chunk.mean()\n",
    "        std = chunk.std()\n",
    "        var = np.square(std)\n",
    "        q1 = np.quantile(chunk, 0.25)\n",
    "        q3 = np.quantile(chunk, 0.75)\n",
    "        arg_sorted = chunk.argsort()\n",
    "        top3 = arg_sorted[-3:]\n",
    "        bottom3 = arg_sorted[:3]\n",
    "        skew = stats.skew(chunk)\n",
    "        kurt = stats.kurtosis(chunk)\n",
    "\n",
    "        minimum = chunk.min()\n",
    "        maximum = chunk.max()\n",
    "\n",
    "        features = [\n",
    "            mean,\n",
    "            std,\n",
    "            q1,\n",
    "            q3,\n",
    "            var,\n",
    "            top3[0],\n",
    "            top3[1],\n",
    "            top3[2],\n",
    "            bottom3[0],\n",
    "            bottom3[1],\n",
    "            bottom3[2],\n",
    "            skew,\n",
    "            kurt,\n",
    "            minimum,\n",
    "            maximum\n",
    "        ]\n",
    "\n",
    "        return features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-04T14:43:28.066246Z",
     "start_time": "2019-04-04T14:40:49.565706Z"
    },
    "deletable": false,
    "editable": false,
    "run_control": {
     "frozen": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((579059603, 2), (50085877, 2))"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train = pd.read_csv(outdir + 'train.csv')\n",
    "valid = pd.read_csv(outdir + 'valid.csv')\n",
    "train.shape, valid.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-04T15:02:15.437014Z",
     "start_time": "2019-04-04T15:02:15.433589Z"
    }
   },
   "outputs": [],
   "source": [
    "def engineer_features(df, step=test_len):\n",
    "    \n",
    "    features = []\n",
    "    labels = []\n",
    "    for i in tqdm(range((len(df) // step) - (test_len // step) + 1)):\n",
    "\n",
    "        cur_slice = df[i * step : i * step + test_len]\n",
    "        time_to_failure = cur_slice['time_to_failure'].values\n",
    "        if time_to_failure[-1] > time_to_failure[0]:\n",
    "            continue\n",
    "\n",
    "        features.append(make_chunk_features(cur_slice['acoustic_data'].values))\n",
    "        labels.append(time_to_failure[-1])\n",
    "\n",
    "    features = np.array(features)\n",
    "    labels = np.array(labels)\n",
    "    \n",
    "    return features, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-04T19:29:21.401851Z",
     "start_time": "2019-04-04T15:02:16.095884Z"
    },
    "deletable": false,
    "editable": false,
    "run_control": {
     "frozen": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Engineering features for train with step 150,000\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b43da0a1bbe7416b94755fd99d585d4b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=3860), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Engineering features for valid with step 150,000\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fef3f0067ec84bb18663916410ff3893",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=333), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Engineering features for train with step 50,000\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a9c705c39f4a4dc5b0910fa6396f586d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=11579), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Engineering features for valid with step 50,000\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1ea7b938cf164131bc5df9b102faf126",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=999), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Engineering features for train with step 10,000\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a4efe52c97784117be006c42f465d5a6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=57891), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Engineering features for valid with step 10,000\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9b822391778946da9db576dec18cbff5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=4994), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "gc.enable()\n",
    "for n in [1, 3, 15]:\n",
    "    for name, data in zip(['train', 'valid'], [train, valid]):\n",
    "        step = test_len // n\n",
    "        print(\"Engineering features for {} with step {:,}\".format(name, step))\n",
    "        features, labels = engineer_features(data, step=step)\n",
    "        \n",
    "        with open(outdir + '{}_features_{}.pkl'.format(name, n), 'wb') as f:\n",
    "            pickle.dump(features, f)\n",
    "\n",
    "        with open(outdir + '{}_labels_{}.pkl'.format(name, n), 'wb') as f:\n",
    "            pickle.dump(labels, f)\n",
    "        \n",
    "        del features, labels\n",
    "        gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Submission data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-04T09:01:18.336613Z",
     "start_time": "2019-04-04T08:52:04.632895Z"
    },
    "deletable": false,
    "editable": false,
    "run_control": {
     "frozen": true
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=1, bar_style='info', max=1), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(2624, 150, 45)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_features = []\n",
    "seg_ids = []\n",
    "test_files = glob.glob(data_dir + 'test/*')\n",
    "for i, f in tqdm(enumerate(test_files)):\n",
    "    df = pd.read_csv(f)\n",
    "    test_features.append(engineer_features(df['acoustic_data'].values))\n",
    "    seg_ids.append(f.split('/')[-1].replace('.csv', ''))\n",
    "    \n",
    "test_features = np.array(test_features)\n",
    "test_features.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-04T09:02:35.921758Z",
     "start_time": "2019-04-04T09:02:35.817390Z"
    },
    "deletable": false,
    "editable": false,
    "run_control": {
     "frozen": true
    }
   },
   "outputs": [],
   "source": [
    "with open(outdir + 'test.pkl', 'wb') as f:\n",
    "    pickle.dump(test_features, f)\n",
    "    \n",
    "with open(outdir + 'test_ids.pkl', 'wb') as f:\n",
    "    pickle.dump(seg_ids, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Machine learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-05T07:28:09.282114Z",
     "start_time": "2019-04-05T07:28:09.277382Z"
    }
   },
   "outputs": [],
   "source": [
    "def load_raw_data(version='1'):\n",
    "\n",
    "    with open(outdir + 'train_features_{}.pkl'.format(version), 'rb') as f:\n",
    "        features_raw = pickle.load(f)\n",
    "\n",
    "    with open(outdir + 'train_labels_{}.pkl'.format(version), 'rb') as f:\n",
    "        labels_raw = pickle.load(f)\n",
    "\n",
    "    with open(outdir + 'valid_features_1.pkl', 'rb') as f:\n",
    "        test_features_raw = pickle.load(f)\n",
    "\n",
    "    with open(outdir + 'valid_labels_1.pkl', 'rb') as f:\n",
    "        test_labels_raw = pickle.load(f)\n",
    "        \n",
    "    return features_raw, labels_raw, test_features_raw, test_labels_raw"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LGBM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Loading data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_raw, labels_raw, test_features_raw, test_labels_raw = load_raw_data(version='3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-05T07:03:15.090156Z",
     "start_time": "2019-04-05T07:03:15.088079Z"
    }
   },
   "outputs": [],
   "source": [
    "def preprocessing_lgbm(features_raw):\n",
    "    \n",
    "    features_1 = features_raw[:, 0, :]\n",
    "    features_2 = features_raw[:, -1, :]\n",
    "    features_3 = features_raw.mean(axis=1)\n",
    "    features = np.concatenate([features_2, features_3], axis=1)    \n",
    "    \n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-05T07:03:16.268811Z",
     "start_time": "2019-04-05T07:03:16.053306Z"
    }
   },
   "outputs": [],
   "source": [
    "features = preprocessing_lgbm(features_raw)\n",
    "test_features = preprocessing_lgbm(test_features_raw)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-05T07:03:23.008059Z",
     "start_time": "2019-04-05T07:03:22.996236Z"
    }
   },
   "outputs": [],
   "source": [
    "def assert_numpy(arr):\n",
    "\n",
    "    if isinstance(arr, np.ndarray):\n",
    "        return arr\n",
    "    elif isinstance(arr, list):\n",
    "        return np.array(arr)\n",
    "    elif isinstance(arr, pd.Series):\n",
    "        return arr.values\n",
    "    else:\n",
    "        print(arr)\n",
    "        raise TypeError('Non array format, can\\'t convert to numpy.')\n",
    "\n",
    "\n",
    "def compute_l1(y_true, preds):\n",
    "    ''' Computes the mean absolute error, or l1 score\n",
    "    Range: [0, inf]\n",
    "    '''\n",
    "    y_true = assert_numpy(y_true)\n",
    "    preds = assert_numpy(preds)\n",
    "\n",
    "    return np.abs(y_true - preds).mean()\n",
    "\n",
    "\n",
    "def compute_l2(y_true, preds):\n",
    "    ''' Computes the mean squared error\n",
    "    Range: [0, inf]\n",
    "    '''\n",
    "    y_true = assert_numpy(y_true)\n",
    "    preds = assert_numpy(preds)\n",
    "\n",
    "    return np.square(y_true - preds).mean()\n",
    "\n",
    "\n",
    "def get_metrics():\n",
    "    \n",
    "    metrics = {\n",
    "        'l1' : compute_l1,\n",
    "        'l2' : compute_l2,\n",
    "    }\n",
    "    \n",
    "    return metrics\n",
    "\n",
    "\n",
    "def ensemble_predict(models, X_test):\n",
    "\n",
    "    preds = np.zeros((len(models), len(X_test)))\n",
    "    for i, m in enumerate(models):\n",
    "        preds[i] = m.predict(X_test)\n",
    "\n",
    "    return preds.mean(axis=0), preds.std(axis=0)\n",
    "\n",
    "\n",
    "def evaluate_model(model, X_valid, Y_valid):\n",
    "    \n",
    "    ## Get predictions\n",
    "    preds = model.predict(X_valid, num_iteration=model.best_iteration_)\n",
    "    \n",
    "    ## Compute metrics such as auc, frequency etc.\n",
    "    metrics = get_metrics()\n",
    "    evals = {}\n",
    "    \n",
    "    for m in metrics.keys():\n",
    "        evals[m] = metrics[m](Y_valid, preds)\n",
    "    \n",
    "    return evals\n",
    "\n",
    "\n",
    "def evaluate_ensemble(models, X_test, Y_test):\n",
    "    \n",
    "    metrics = get_metrics()\n",
    "    eval_lists = {m : [] for m in metrics.keys()}\n",
    "    for model in models:\n",
    "        evals = evaluate_model(model, X_test, Y_test)\n",
    "        for m in evals.keys():\n",
    "            eval_lists[m].append(evals[m])\n",
    "            \n",
    "    mean_evals = {m : np.mean(eval_lists[m]) for m in eval_lists.keys()}\n",
    "    \n",
    "    return mean_evals\n",
    "    \n",
    "\n",
    "def train_ensemble(lgbparams, features, targets, nfold=10, test_size=0.1):\n",
    "    \n",
    "    kfold = KFold(n_splits=nfold, shuffle=False)\n",
    "    metrics = get_metrics()\n",
    "    eval_lists = {m : [] for m in metrics.keys()}\n",
    "    feature_importances_df = pd.DataFrame()\n",
    "    models = []\n",
    "\n",
    "    for i, (train_index, test_index) in enumerate(kfold.split(features, targets)):\n",
    "        \n",
    "        ## Extract training and validation data\n",
    "        X_train, X_valid = features[train_index], features[test_index]\n",
    "        Y_train, Y_valid = targets[train_index], targets[test_index]\n",
    "\n",
    "        ## Train lgb model\n",
    "        model = lgb.LGBMRegressor(**lgbparams)\n",
    "        model.fit(X_train, Y_train, eval_metric=lgbparams['metric'],\n",
    "                      eval_set=[(X_valid, Y_valid), (X_train, Y_train)],\n",
    "                      eval_names=['valid', 'train'],\n",
    "                      early_stopping_rounds=100, verbose=False)\n",
    "        \n",
    "        ## Evaluate model\n",
    "        evals = evaluate_model(model, X_valid, Y_valid)\n",
    "        \n",
    "        for m in evals.keys():\n",
    "            eval_lists[m].append(evals[m])\n",
    "            \n",
    "        models.append(model)\n",
    "    \n",
    "    mean_evals = {m : np.mean(eval_lists[m]) for m in eval_lists.keys()}\n",
    "    \n",
    "    return models, mean_evals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-05T07:03:23.378632Z",
     "start_time": "2019-04-05T07:03:23.373481Z"
    }
   },
   "outputs": [],
   "source": [
    "lgbparams = dict(\n",
    "    random_state = 1,\n",
    "    objective = 'regression_l1',\n",
    "    metric = 'mae',\n",
    "    learning_rate = 0.3,\n",
    "    num_leaves = 17,\n",
    "    max_depth = 9,\n",
    "    num_boosting_rounds = 10000,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-05T07:03:30.706243Z",
     "start_time": "2019-04-05T07:03:24.018434Z"
    }
   },
   "outputs": [],
   "source": [
    "models, evals = train_ensemble(lgbparams, features, labels_raw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-05T07:03:30.730576Z",
     "start_time": "2019-04-05T07:03:30.707406Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'l1': 2.0157322850187027, 'l2': 6.03985328628869}"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluate_ensemble(models, test_features, test_labels_raw)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-04T09:11:02.955263Z",
     "start_time": "2019-04-04T09:11:02.863094Z"
    }
   },
   "outputs": [],
   "source": [
    "with open(outdir + 'test.pkl'.format(version), 'rb') as f:\n",
    "    test_features = pickle.load(f)\n",
    "    \n",
    "with open(outdir + 'test_ids.pkl'.format(version), 'rb') as f:\n",
    "    ids = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-04T09:11:03.273779Z",
     "start_time": "2019-04-04T09:11:03.241095Z"
    }
   },
   "outputs": [],
   "source": [
    "test_features = test_features.mean(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-04T09:11:05.819045Z",
     "start_time": "2019-04-04T09:11:05.791086Z"
    }
   },
   "outputs": [],
   "source": [
    "preds, _ = ensemble_predict(models, test_features)\n",
    "preds = pd.DataFrame({'seg_id' : ids, 'time_to_failure' : preds})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-04T09:17:29.908966Z",
     "start_time": "2019-04-04T09:17:29.899735Z"
    }
   },
   "outputs": [],
   "source": [
    "preds.to_csv('../submission.csv', index=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-05T07:22:54.225241Z",
     "start_time": "2019-04-05T07:22:54.223111Z"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Preprocess data for LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-05T07:28:46.815468Z",
     "start_time": "2019-04-05T07:28:46.414459Z"
    }
   },
   "outputs": [],
   "source": [
    "features_raw, labels_raw, test_features_raw, test_labels_raw = load_raw_data(version='3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-05T07:28:47.910019Z",
     "start_time": "2019-04-05T07:28:47.714983Z"
    }
   },
   "outputs": [],
   "source": [
    "features = features_raw.copy()\n",
    "labels = labels_raw.copy()\n",
    "\n",
    "test_features = test_features_raw.copy()\n",
    "test_labels = test_labels_raw.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-05T07:28:48.309626Z",
     "start_time": "2019-04-05T07:28:48.298666Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def truncate_features(features, chunk_size=10):\n",
    "    new_features = []\n",
    "    chunk_size = 10\n",
    "    for f in features:\n",
    "        new_f = []\n",
    "        for s in range(features.shape[1] // chunk_size):\n",
    "            f_s = f[s*chunk_size : (s+1)*chunk_size].mean(axis=0)\n",
    "            new_f.append(f_s)\n",
    "        new_features.append(new_f)\n",
    "\n",
    "    new_features = np.array(new_features)\n",
    "    \n",
    "    return new_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-05T07:28:50.163109Z",
     "start_time": "2019-04-05T07:28:49.083168Z"
    }
   },
   "outputs": [],
   "source": [
    "## Make smaller\n",
    "features = truncate_features(features)\n",
    "test_features = truncate_features(test_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-05T07:28:51.698558Z",
     "start_time": "2019-04-05T07:28:51.695982Z"
    }
   },
   "outputs": [],
   "source": [
    "def scale_features(features, scaler=None):\n",
    "    \n",
    "    prev_shape = features.shape\n",
    "    features = features.reshape(-1, features.shape[-1])\n",
    "    if scaler is None:\n",
    "        scaler = MinMaxScaler()\n",
    "        scaler.fit(features)\n",
    "    features = scaler.transform(features)\n",
    "    features = features.reshape(prev_shape)\n",
    "    \n",
    "    return features, scaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-05T07:14:47.975536Z",
     "start_time": "2019-04-05T07:14:47.954005Z"
    }
   },
   "outputs": [],
   "source": [
    "## Scale\n",
    "features, scaler = scale_features(features)\n",
    "test_features, scaler = scale_features(test_features, scaler)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-04T11:33:32.682422Z",
     "start_time": "2019-04-04T11:33:32.680981Z"
    }
   },
   "source": [
    "#### Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-05T07:28:53.521259Z",
     "start_time": "2019-04-05T07:28:53.518424Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_loader(feats, labels, shuffle=False, batch_size=32):\n",
    "\n",
    "    features = torch.tensor(feats, dtype=torch.float32)\n",
    "    labels = torch.tensor(labels, dtype=torch.float32)\n",
    "    dataset = TensorDataset(features, labels)\n",
    "    data_loader = DataLoader(dataset, shuffle=shuffle, batch_size=batch_size)\n",
    "    \n",
    "    return data_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-05T07:28:53.859880Z",
     "start_time": "2019-04-05T07:28:53.819890Z"
    }
   },
   "outputs": [],
   "source": [
    "X_train, X_valid, Y_train, Y_valid = train_test_split(features, labels, test_size=0.1, shuffle=False)\n",
    "train_loader = get_loader(X_train, Y_train, shuffle=True, batch_size=32)\n",
    "valid_loader = get_loader(X_valid, Y_valid, shuffle=False, batch_size=128)\n",
    "test_loader = get_loader(test_features, test_labels, shuffle=False, batch_size=128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-05T07:31:18.393247Z",
     "start_time": "2019-04-05T07:31:18.384394Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10382, 15, 45)"
      ]
     },
     "execution_count": 138,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_data, seq_len, n_features = X_train.shape\n",
    "X_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Custom loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-04T12:17:36.967987Z",
     "start_time": "2019-04-04T12:16:13.700591Z"
    },
    "deletable": false,
    "editable": false,
    "run_control": {
     "frozen": true
    }
   },
   "outputs": [],
   "source": [
    "import importlib \n",
    "import utils.dataset\n",
    "importlib.reload(utils.dataset)\n",
    "from utils.dataset import *\n",
    "\n",
    "train_data = train.values\n",
    "del train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 287,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-04T13:43:33.291552Z",
     "start_time": "2019-04-04T13:43:33.286066Z"
    },
    "deletable": false,
    "editable": false,
    "run_control": {
     "frozen": true
    }
   },
   "outputs": [],
   "source": [
    "seq_len, n_features = engineer_features(train_data[0:test_len, 0]).shape\n",
    "\n",
    "X_train = train_data[second_earthquake + 1:]\n",
    "X_valid = train_data[:second_earthquake + 1]\n",
    "\n",
    "train_step = 50000\n",
    "batch_size = 32\n",
    "train_dataset = EarthquakeDatasetTrain(X_train, window_step=train_step, mask_prob=0.0)\n",
    "valid_dataset = EarthquakeDatasetTrain(X_valid, window_step=150000)\n",
    "\n",
    "train_loader = RandomLoader(train_dataset, \n",
    "                            batch_size=batch_size,\n",
    "                            num_epoch_steps=int(len(X_train) / train_step / batch_size))\n",
    "\n",
    "valid_loader = DataLoader(valid_dataset, \n",
    "                          batch_size=64, \n",
    "                          shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-05T07:28:56.519177Z",
     "start_time": "2019-04-05T07:28:56.515226Z"
    }
   },
   "outputs": [],
   "source": [
    "class LSTMNet(nn.Module):\n",
    "    \n",
    "    def __init__(self, config):\n",
    "        super(LSTMNet, self).__init__()\n",
    "        \n",
    "        self.config = config\n",
    "        self.device = torch.device(\"cuda\" if self.config['use_cuda'] else \"cpu\")\n",
    "        self.rnn = nn.GRU(config['n_features'], config['hidden_size'],\n",
    "            batch_first=True,\n",
    "            num_layers=config['num_layers'], \n",
    "            bidirectional=config['bidirectional'],\n",
    "            dropout=config['dropout'])\n",
    "        lstm_outsize = config['hidden_size'] * (1+1*config['bidirectional'])\n",
    "        self.dropout = nn.Dropout(config['dropout'])\n",
    "        self.dense = nn.Linear(lstm_outsize, config['dense_size'])\n",
    "        self.classifier = nn.Linear(config['dense_size'], 1)\n",
    "        self.criterion = nn.L1Loss()\n",
    "        \n",
    "    def forward(self, x, labels=None):\n",
    "        x, hidden = self.rnn(x)\n",
    "        x = self.dropout(x[:, -1, :])\n",
    "        x = self.dropout(nn.functional.relu(self.dense(x)))\n",
    "        x = self.classifier(x)\n",
    "\n",
    "        if labels is not None:\n",
    "            return x, self.criterion(x, labels)\n",
    "        else:\n",
    "            return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-05T07:29:17.407025Z",
     "start_time": "2019-04-05T07:29:17.403444Z"
    }
   },
   "outputs": [],
   "source": [
    "config = dict(\n",
    "    use_cuda = torch.cuda.is_available(),\n",
    "    seq_len = seq_len,\n",
    "    n_features = n_features,\n",
    "    \n",
    "    lr = 0.005,\n",
    "    num_layers = 1,\n",
    "    bidirectional = False,\n",
    "    hidden_size = 256,\n",
    "    dropout = 0.3,\n",
    "    dense_size = 256,\n",
    ")\n",
    "device = torch.device('cuda' if config['use_cuda'] else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-05T07:29:18.514231Z",
     "start_time": "2019-04-05T07:29:18.509470Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "298,753 parameters\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aiwizo/anaconda3/lib/python3.6/site-packages/torch/nn/modules/rnn.py:46: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.3 and num_layers=1\n",
      "  \"num_layers={}\".format(dropout, num_layers))\n"
     ]
    }
   ],
   "source": [
    "model = LSTMNet(config).to(device)\n",
    "print('{:,} parameters'.format(sum([np.prod(param.shape) for param in model.parameters()])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-05T07:29:21.372489Z",
     "start_time": "2019-04-05T07:29:21.370777Z"
    }
   },
   "outputs": [],
   "source": [
    "params = model.parameters()\n",
    "optimizer = torch.optim.Adam(params, lr=config['lr'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-05T07:29:22.509731Z",
     "start_time": "2019-04-05T07:29:22.506989Z"
    }
   },
   "outputs": [],
   "source": [
    "def eval_loader(model, loader):\n",
    "    \n",
    "    model.eval()\n",
    "    tot_loss = tot_examples = 0\n",
    "    with torch.no_grad():\n",
    "        for batch in loader:\n",
    "            \n",
    "            if isinstance(batch, dict):\n",
    "                features = batch['features'].to(device)\n",
    "                labels = batch['labels'].to(device)\n",
    "            else:\n",
    "                features = batch[0].to(device)\n",
    "                labels = batch[1].to(device)\n",
    "                \n",
    "            batch_size = len(features)\n",
    "\n",
    "            output, loss = model(features, labels)\n",
    "\n",
    "            tot_loss += loss.cpu().item() * batch_size\n",
    "            tot_examples += batch_size\n",
    "        \n",
    "    model.train()\n",
    "    \n",
    "    return tot_loss / tot_examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-05T07:29:23.033108Z",
     "start_time": "2019-04-05T07:29:23.018604Z"
    }
   },
   "outputs": [],
   "source": [
    "def train(model, train_loader, valid_loader, num_epochs=20, progress_bar=False):\n",
    "    model.train()\n",
    "    print(\"loss before training:\")\n",
    "    print(eval_loader(model, valid_loader))\n",
    "    print()\n",
    "    for e in range(num_epochs):\n",
    "        if progress_bar:\n",
    "            pbar = tqdm(total=len(train_loader))\n",
    "        for batch in train_loader:\n",
    "            \n",
    "            if isinstance(batch, dict):\n",
    "                features = batch['features'].to(device)\n",
    "                labels = batch['labels'].to(device)\n",
    "            else:\n",
    "                features = batch[0].to(device)\n",
    "                labels = batch[1].to(device)\n",
    "\n",
    "            output, loss = model(features, labels)\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            if progress_bar:\n",
    "                pbar.update(1)\n",
    "        \n",
    "        print(\"loss after epoch {}:\".format(e+1))\n",
    "        print(eval_loader(model, valid_loader))\n",
    "        print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-05T07:29:35.753429Z",
     "start_time": "2019-04-05T07:29:23.762779Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss before training:\n",
      "4.818936448783147\n",
      "\n",
      "loss after epoch 1:\n",
      "3.2542179064692824\n",
      "\n",
      "loss after epoch 2:\n",
      "2.980502081495844\n",
      "\n",
      "loss after epoch 3:\n",
      "2.8848164193138506\n",
      "\n",
      "loss after epoch 4:\n",
      "3.0910143009298183\n",
      "\n",
      "loss after epoch 5:\n",
      "3.1977603530553225\n",
      "\n",
      "loss after epoch 6:\n",
      "2.9255593356061436\n",
      "\n",
      "loss after epoch 7:\n",
      "3.0660766596604057\n",
      "\n",
      "loss after epoch 8:\n",
      "3.0744742743890456\n",
      "\n",
      "loss after epoch 9:\n",
      "3.2512747319980138\n",
      "\n",
      "loss after epoch 10:\n",
      "3.4239871134171245\n",
      "\n",
      "loss after epoch 11:\n",
      "3.085137952345083\n",
      "\n",
      "loss after epoch 12:\n",
      "3.1231004369527464\n",
      "\n",
      "loss after epoch 13:\n",
      "3.3154710045720304\n",
      "\n",
      "loss after epoch 14:\n",
      "3.113197033285475\n",
      "\n",
      "loss after epoch 15:\n",
      "3.1412166210353063\n",
      "\n",
      "loss after epoch 16:\n",
      "2.884876691695929\n",
      "\n",
      "loss after epoch 17:\n",
      "3.2580325830747183\n",
      "\n",
      "loss after epoch 18:\n",
      "3.212992804509102\n",
      "\n",
      "loss after epoch 19:\n",
      "3.1762593127206045\n",
      "\n",
      "loss after epoch 20:\n",
      "3.27195335715441\n",
      "\n"
     ]
    }
   ],
   "source": [
    "train(model, train_loader, valid_loader, num_epochs=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-05T07:29:35.763653Z",
     "start_time": "2019-04-05T07:29:35.754729Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3.1881400935621147"
      ]
     },
     "execution_count": 137,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eval_loader(model, test_loader)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
